---
engine: julia
---

:::{.content-visible when-format="pdf"}
\HeaderNumbered
:::


:::{.hidden}
{{< include mathematics.qmd >}}

```{julia}
#| echo: false
#| output: false

using Pkg
Pkg.activate(".")
```
:::

:::{.content-hidden unless-format="html"}
```{julia}
#| echo: false
#| output: false

using WGLMakie
WGLMakie.activate!()
```
:::

:::{.content-hidden when-format="html"}
```{julia}
#| echo: false
#| output: false

using CairoMakie
CairoMakie.activate!()
```
:::

# Bifurcation Theory Approach

As we have mentioned several times before, the fundamental goal of this thesis is to study the computational aspects of continuation/detection of stable and unstable manifolds of periodic orbits (and what challenges come with it). As such, we shall properly discuss what a continuation is and how we compute it. We begin with a theoretical treatment of bifurcation theory, focused solely on cycle continuation and stability identification, followed by original results concerning coupled models of neurons. The theoretical introduction is mainly based on @Kuznetsov2023, @Guo2013, and @Smith2010, but many more related articles are cited throughout the discussion.

> We should also disclose that while this chapter is named 'Bifurcation Theory Approach', we will spend relatively little time discussing actual bifurcations. Our main focus lies in the continuation of (un)stable cycles.

<!-- TODO: Change the title -->
## Theory of Localizations

Although the primary subjects of our study are periodic orbits, we will first delve into the localization of equilibrium. Indeed, one might recall we discussed the relation of equilibria (or fixed points) in discrete dynamical systems and periodic orbits of continuous-time dynamical systems in @sec-poincare-map. This fact alone suggests it is beneficial to first study the simpler case of equilibria localization, building tools to later tackle the periodic case.

### Dynamical Systems

#### Equilibrium Localization

To start relatively simple, consider a continuous-time dynamical system induced by an autonomous ODE of form ([-@eq-autonomous-ode-system]), i.e.
$$
\dvi x = \vi f(\vi x), \; \vi x \in \R^n.
$$
Its equilibrium $\vi x^*$ is then given by
$$
\vi f(\vi x^*) = 0.
$$ {#eq-equilibrium-cond}
Note that similarly by setting $\vi f(\vi x) = \vi g(\vi x) - \vi x$ we can get the same equilibrium condition for a discrete-time dynamical system $\vi x \mapsto \vi g(\vi x)$.

It is easy to see that if the equilibrium $\vi x^*$ is stable, one can integrate the system ([-@eq-autonomous-ode-system]) forward in time starting from some point $\vi x$ in the basis of attraction of $\vi x^*$, since
$$
\lim_{t \to \infty} \norm{\vi \evolOp(t, \vi x) - \vi x^*} = 0
$$
by @def-invariant-set-stable. Similarly, if $\vi x^*$ is *totally unstable* (i.e. *repelling* from all directions), we can simply reverse time (as we are in the ODE case) and repeat the procedure.

Nonetheless, in general, this approach will not work, as equilibria are often neither stable nor repelling. The standard procedure is to start from a point in a small neighborhood around the equilibrium (determined either analytically or via numerical integration) and construct a sequence of point $\Seqnc {\vi x^{(i)}} {i = 0} \infty$, which converges to $\vi x^*$ under general conditions. For this purpose, we can use the Newton's method, see @sec-newton-method, or one of its modifications.

Suppose we have found $\vi x^*$ with desired accuracy. Our next task is then to determine its stability, i.e. how the phase portrait behaves in its vicinity. By @thm-lyapunov-stability-theorem, @thm-grobman-hartman-cont and the following discussion, we know that eigenvalues of the Jacobian matrix $\Jacobi^*$ determine the stability of $\vi x^*$. In other words, we need to compute the roots of the *characteristic polynomial*
$$
p(\lmbd) = \det (\Jacobi^* - \lmbd \ident).
$$
Note that there are other methods of computing stability, which in particular do not require eigenvalues, but only computation of certain determinants of $\Jacobi^*$. We refer the interested reader to @Kuznetsov2023, page 470.

#### Periodic Orbit Localization

As we have seen, localization of an equilibrium from sufficiently close initial guess was rather simple both theoretically and computationally. Unfortunately, for limit cycles the situation is more complicated. For clarity, @Kuznetsov2016 was heavily used as a source material for this section.

Assume again we have a dynamical system prescribed by ([-@eq-autonomous-ode-system]), 
$$
\dvi x = \vi f(\vi x), \; \vi x \in \R^n,
$$
such that is has an isolated periodic orbit $L_0$. Let $\vi x^0(t + T_0) = \vi x^0(t)$ be the corresponding solution with minimal period $T_0 > 0$. Further assume that the cycle $L_0$ has corresponding multipliers, see @sec-poincare-map and @lem-poincar√©-map, 
$$
\mu_1, \dots, \mu_n \in \C,
$$
which are by @thm-floquet-exponents the eigenvalues of the $n\times n$ monodromy matrix $\vi M(T_0)$, where $\vi M(t)$ satisfies
$$
\rcases{
	\dvi M(t) &= \jacobi \vi f (t) \vi M(t), \\
	\vi M(0) &= \ident_n.
}
$$ {#eq-ode-monodromy-system}
Recall there is always a trivial multiplier $\mu_1 = 1$. Also, if $\absval{\mu_i} < 1$ for all $i \in \oneToN{n}$, then the cycle is stable. In contrast, if there exists $i \in \oneToN{n}$ such that $\absval{\mu_i} > 1$, the cycle is unstable (and if all multipliers have modulus greater than one, the cycle is called *totally unstable* or *repelling*).

If the system features a stable limit cycle $L_0$, then we can find it again by numerical integration from a point in its basin of attraction. In contrast to the case of equilibria, backward-time numerical integration will not, in general, help us find a repelling periodic orbit. 

Suppose we have a system in 2-dimensional state space, which possesses a periodic orbit, see @fig-2d-periodic-orbit. From the theory of differential equations, we know that in the case of an autonomous ODE ([-@eq-autonomous-ode-system]) its trajectories do not intersect. Thus a periodic orbit $L_0$ constitutes a Jordan curve^[By ([-@eq-autonomous-ode-system]) and the periodicity constraint, there exists a map $\vi \phi(t) = \evolOp(t, \vi x(t))$ such that it maps the interval $[t_0, t_0 + T_0]$ onto the cycle $L_0$ and is surely continuous by @def-dynamical-system and @def-autonomous-ode-system for sufficiently smooth $\vi f$ (otherwise $\vi f$ would not give rise to a dynamical system in the first place). Moreover, $\vi \phi(t)$ is injective on $[t_0, t_0 + T_0)$ as there can be no intersections. In total, $L_0 = \vi \phi([t_0, t_0 + T_0])$ is a Jordan curve.]. Then by Jordan curve theorem, see @wikiJordanCurve, such orbit partitions the state space into inside and outside of the cycle (and the cycle itself)^[In practice, it suffices to consider a sufficiently large connected region containing the cycle instead of the entire state space]. Thus if $L_0$ is repelling, trajectories are guaranteed to converge to $L_0$ backward in time.

![Unstable periodic orbit $L_0$ of a 2-dimensional dynamical system.](diagrams/2D-state-space-periodic-orbit.drawio.svg){#fig-2d-periodic-orbit .final}

On the other hand, for 3- or more dimensional dynamical systems, a periodic orbit does not partition the state space (or any local neighborhood). Then, we cannot guarantee a convergence to the repelling cycle by backward-time integration no matter the initial vicinity. Moreover, it is needless to say the numerical integration localization approach will surely fail if the cycle is not stable nor repelling.

From now on, we will assume we know the location of the periodic orbit approximately. A correction method, most often Newton-Raphson, is then applied repeatedly until satisfactory convergence. This is a rather common scenario in practice, where we often know the location of the cycle for a given parameter value and wish to "continue" by varying the parameter and correcting the cycle afterwards. This process is commonly referred to as *continuation*.

Usually, we take the period $T$ of $L_0$ as an unknown and formulate the *boundary-value problem* (BVP) on a fixed interval. In particular, assume $T$ is a parameter and construct a "time-normalized" system
$$
\deriv {\vi u} {s} = T \vi f(\vi u), \; s \in [0,1],
$$ {#eq-autonomous-ode-period-1}
which rescales ([-@eq-autonomous-ode-system]) by a time-scaling factor $T$, such that the new time is denoted $s$. Now, if a solution $\vi u(s)$ to ([-@eq-autonomous-ode-period-1]) also satisfies the *periodic boundary conditions*
$$
\vi u(0) = \vi u(0),
$$ {#eq-autonomous-ode-1-periodicity-cond}
then it corresponds to a $T$-periodic solution of ([-@eq-autonomous-ode-system]). However, these two conditions do not prescribe the period orbit of ([-@eq-autonomous-ode-system]) uniquely. Indeed, any time shift of the solution to the BVP ([-@eq-autonomous-ode-period-1]), ([-@eq-autonomous-ode-1-periodicity-cond]) also determines the same periodic orbit.

Therefore, an extra condition, called the *phase condition*, must be added. Such condition is most often written in the form
$$
\Phi[\vi u] = 0,
$$ {#eq-general-phase-condition}
where $\Phi$ is a scalar *functional* defined on the space of periodic solutions. The exact choice of phase condition is up to the user, but the most frequent one is the *integral phase condition*
$$
\Phi[\vi u] = \int_0^1 \scal {\vi u(s)} {\dvi v(s)} \dd s,
$$ {#eq-integral-phase-cond}
where $\vi v(s) \in \contf{[0, 1], \R^n}{}$ is the reference period-one solution.

:::{#lem-integral-phase-cond}
The integral phase condition ([-@eq-integral-phase-cond]) is a necessary condition for a local minimum of the time-shift $L_2$-distance of period-1 smooth functions $\vi u, \vi v : \R \to \R^n$
$$
\rho(\sigma) = \int_0^1 \norm{\vi u(s + \sigma) - \vi v(s)}_2^2 \dd s,
$$
such that the minimum is obtained at shift $\sigma = 0$.
:::

:::{.proof}
See @Kuznetsov2016, lemma 11.
:::

We can now collect all the conditions to obtain the periodic BVP
$$
\rcases{
	\dvi u(s) &= T \vi f(\vi u(s)),\; s \in [0,1] \\
	\vi u(0) &= \vi u(1), \\
	\int_0^1 \scal {\vi u(s)} {\dvi v(s)} \dd s &= 0.
}
$$ {#eq-ode-cycle-bvp}
If $(\vi u(\cdot), \vi T_0) \in \contf {[0,1], \R^n} 1 \times \R$ satisfies ([-@eq-ode-cycle-bvp]), then $\vi x(t) = \vi u\brackets{\frac t {T_0}}$ gives the $T_0$-periodic solution of ([-@eq-autonomous-ode-system]) with $\vi x(0) = \vi u(0)$.

Moreover, for the fundamental matrix solution $\vi M(t)$ of ([-@eq-autonomous-ode-system]) we define $\vi \Phi(s)$, such that for the monodromy matrix of $L_0$ holds $\vi M(T) = \vi \Phi(1)$ and
$$
\dvi \Phi(s) - T \jacobi \vi f(\vi u(s)) \vi \Phi(s) = \vi 0, \; \vi \Phi(0) = \vi \ident_n.
$$ {#eq-monodromy-system-normalized}
The eigenvalues of $\vi \Phi(1)$ correspond exactly to the aforementioned multipliers of the cycle $L_0$. We also define the **adjoint monodromy matrix** $\vi \Psi(1)$ as the solution of
$$
\dvi \Psi(s) - T \jacobi \vi f\Tr(\vi u(s)) \vi \Psi(s) = \vi 0, \; \vi \Psi(0) = \vi \ident_n
$$
evaluated at 1. It follows that
$$
\vi \Psi(s) = \brackets{\vi \Phi(s)\Inv}\Tr.
$$ {#eq-monodromy-psi-phi}
In general, any solution $\vi \xi \in \contf {[0,1], \R^n} {1}$ of an inhomogeneous linear system
$$
\dvi \xi - T \jacobi \vi f(\vi u(s)) \vi \xi = \vi b(s),
$$ 
where $\vi b \in \contf {\R, \R^n} {0}$, can be written as (by ([-@eq-monodromy-psi-phi]))^[In other words, $\vi \Phi(s)$ is a *fundamental matrix* of ([-@eq-monodromy-system-normalized]). Moreover, this viewpoint explains ([-@eq-monodromy-variation-of-const]) as a corollary of method of variation of constants.]
$$
\vi \xi(s) = \vi \Phi(s) \brackets{\vi \xi(0) + \int_0^{s} \vi \Phi\Inv(\sigma) \vi b(\sigma) \dd \sigma} = \vi \Phi(s) \brackets{\vi \xi(0) + \int_0^s \vi \Psi\Tr(\sigma) \vi b(\sigma) \dd \sigma}.
$$ {#eq-monodromy-variation-of-const}

:::{#def-simple-cycle}
A cycle $L$ is called **simple** if the trivial multiplier $\mu_1 = 1$ has algebraic multiplicity^[Recall the algebraic multiplicity of an eigenvalue $\lmbd$ of a matrix $\vi A$ is its multiplicity as a root of characteristic polynomial. Furthermore, geometric multiplicity of $\lmbd$ is defined as $\dim \ker (\vi A - \lmbd \ident)$. The algebraic multiplicity is always greater than or equal to the geometric multiplicity for any given eigenvalue $\lmbd$ of $\vi A$.] 1.
:::

Let $\vi q_0, \vi p_0 \in \R^n$ be the left and right eigenvectors of the monodromy matrix corresponding to the trivial multiplier $\mu_1$,
$$
\begin{aligned}
(\vi \Phi(1) - \ident_n) \vi q_0 = (\vi \Psi(1) - \ident_n) \vi p_0 & = \vi 0, \\
(\vi \Phi(1) - \ident_n)\Tr \vi p_0 = (\vi \Psi(1) - \ident_n)\Tr \vi q_0 & = \vi 0,
\end{aligned}
$$ 
such that $\norm{\vi p_0}_2 = \norm{\vi q_0}_2 = 1$. Also, $\vi q_0 = c_0 \vi f(\vi u(0))$ for $c_0 \in \R \setminus \set{0}$, which follows from ([-@eq-monodromy-system-normalized]) using ([-@eq-autonomous-ode-period-1]),([-@eq-autonomous-ode-1-periodicity-cond]):
\begin{align*}
	\partialOp {s} \brackets{c_0 \vi f(\vi u(0))} - T \vi f(\vi u(1)) c_0 \vi f(\vi u(0)) &= c_0 \jacobi \vi f(\vi u(0)) \dvi u(0) - T \jacobi \vi f(\vi u(0)) c_0 \vi f(\vi u(0)) \\	
	&= c_0 \jacobi \vi f(\vi u(0)) T \vi f(\vi u(0)) - T \jacobi \vi f(\vi u(0)) c_0 \vi f(\vi u(0)) \\
	&= \vi 0.
\end{align*} 

As we have mentioned, the workflow for localization of a periodic orbit is to start from an initial guess $(\vi u, T)$, which we correct by $(\vi w, S) \in \contf {[0,1], \R^n} {1} \times \R$, i.e. 
$$
(\vi u, T) \mapsto (\vi u + \vi w, T + S),
$$
where $(\vi w(\cdot), S)$ is the solution of the linearized inhomogeneous BVP (for reference see ([-@eq-ode-cycle-bvp]))
$$
\rcases{
	\dvi w(s) - T \jacobi \vi f(\vi u(s)) \vi w - S \vi f(\vi u(s)) &= - \dvi u(s) + T \vi f(\vi u(s)), \; s \in [0,1], \\
	\vi w(0) - \vi w(1) &= - \vi u(1) + \vi u(0), \\
	\int_0^1 \scal {\dvi v(\sigma)} {\vi w(\sigma)} \dd \sigma &= - \int_0^1 \scal {\dvi v(\sigma)} {\vi u(\sigma)} \dd \sigma.
}
$$ {#eq-newton-step-bvp}
The left-hand side of ([-@eq-newton-step-bvp]) can be expressed as a matrix operator of form
$$
\underbrace{\bmtr{
	\oper{\jacobi} - T \jacobi \vi f(\vi u) & -\vi f(\vi u) \\
	\evalOp{0} - \evalOp{1} & 0 \\
	\testInt{\dvi v} & 0
}}_{\oper L_{\vi u, T}} \mtr{
	\vi w \\ S
},
$$
where $\oper{\jacobi}$ denotes the differentiation operator, $\evalOp{a}$ is the evaluation operator at $t = a$, i.e. $\evalOp{a} \vi w = \vi w(a)$, and
$$
\testInt{\dvi v} \vi w = \int_0^1 \scal{\dvi v(\sigma)} {\vi w(\sigma)} \dd \sigma.
$$

By taking $\vi v = \vi u$, where $\vi u$ is a solution of ([-@eq-ode-cycle-bvp]), we get $\dvi v = \dvi u = T \vi f(\vi u)$. Moreover, for the purposes of the following theorem, one might choose $\vi v$ sufficiently close to such $\vi u$ (most importantly when $\vi v$ is a reference period-1 function, for example the solution in the previous step of continuation). Lastly, replacement of $T \vi f(\vi u)$ by $\vi f(\vi u)$ does not change the fundamental properties of the operator $\oper{L}_{\vi u, T}$.

:::{#thm-ode-correction-operator-bijection}
If $(\vi u(\cdot), T)$ corresponds to a simple cycle, then the operator
$$
\oper{L}_{\vi u, T} = \bmtr{
	\oper{\jacobi} - T \jacobi \vi f(\vi u) & - \vi f(\vi u) \\
	\evalOp{1} - \evalOp{0} & 0 \\
	\testInt{\vi f(\vi u)} & 0
},
$$
from $\contf {[0,1], \R^n} {1} \times \R$ into $\contf {[0,1], \R^n} {1} \times \R^n \times \R$ is one-to-one and onto.
:::

:::{.proof}
See @Kuznetsov2016, page 36.
:::

By @thm-ode-correction-operator-bijection we know ([-@eq-newton-step-bvp]) can be collectively rewritten as
$$
\oper{L}_{\vi u, T} \mtr{\vi w \\ S} = \vi A_{\vi u, T},
$$ {#eq-newton-correction-infinite-dim}
where $\vi A_{\vi u, T}$ captures the right-hand side of ([-@eq-newton-step-bvp]), and that $\oper{L}_{\vi u, T}$ is regular. Thus, an appropriate correction $(\vi w, S)$ can indeed be found by Newton's method, see @sec-newton-method (assuming we can find a suitable discretization, as ([-@eq-newton-correction-infinite-dim]) is in fact infinite-dimensional).

This approach can be extended to the **continuation** of a **limit cycle branch** of a system
$$
\dvi x = \vi f(\vi x, \alpha), \; \vi x \in \R^n, \; \alpha \in \R,
$$ {#eq-cycle-continuation-problem}
with respect to (w.r.t.) parameter $\alpha \in \R$ as the solution to the following BVP ($\vi u_{\text{old}}$ denotes the period-1 cycle for the previous value of $\alpha$):
$$
\rcases{
	\dvi u(s) - T \vi f(\vi u(s), \alpha) &= \vi 0, \; s \in [0,1], \\
	\vi u(0) - \vi u(1) &= \vi 0, \\
	\int_0^1 \scal{\vi u(\sigma)} {\dvi u_{\text{old}}(\sigma)} \dd \sigma &= 0.
}
$$ {#eq-ode-continuation-bvp}
Furthermore, from @thm-ode-correction-operator-bijection and implicit function theorem, we obtain that a simple cycle has *locally unique* continuation w.r.t. parameter $\alpha$. Again, we can define a corresponding operator to ([-@eq-ode-continuation-bvp]),
$$
\bmtr{
	\oper{\jacobi}_{\vi u} - T \jacobi_{\vi u} \vi f(\vi u, \alpha) & - \vi f(\vi u, \alpha) & - T \pDeriv {\vi f} {\alpha}(\vi u, \alpha) \\
	\evalOp{0} - \evalOp{1} & 0 & 0 \\
	\testInt{\dvi u_{\text{old}}} & 0 & 0
},
$$ {#eq-ode-continuation-operator}
which then has a one-dimensional null-space for a simple cycle $\vi u$.

To solve ([-@eq-ode-cycle-bvp]) (or ([-@eq-ode-continuation-bvp])) numerically, we have to reduce it to a finite-dimensional problem (as opposed to searching in the infinite-dimensional space of periodic functions $\vi u$), i.e. we need to choose a *discretization*. Although shooting, multiple shooting and finite differences are sometimes used, most common and most capable is the method of *orthogonal collocation*.

Consider for simplicity the BVP ([-@eq-ode-cycle-bvp]). We shall introduce a partitioning of the interval $[0,1]$ by $N - 1$ mesh points, i.e.
$$
0 = s_0 < s_1 < \dots < s_N = 1.
$$
The primary goal of the orthogonal collocation is to approximate the true solution $\vi u$ by piecewise-differentiable continuous function, which is defined as a *vector polynomial* $\vi u^{(j)}(s)$ of maximal degree $m$ within each subinterval $[s_j, s_{j+1}]$, $j = 0, 1, \dots, N-1$. Such polynomials can be specified by $m$ points 
$$
s_j < \zeta_{j,1} < \zeta_{j,2} < \dots < \zeta_{j,m} < s_{j+1}
$$
belonging to each subinterval, where the approximate solution must satisfy the time-normalized ODE system ([-@eq-autonomous-ode-period-1]) exactly. That is, we require
$$
\evaluateAt{\deriv {\vi u^{(j)}} s} {s = \zeta_{j,i}} = T \vi f(\vi u^{(j)}(\zeta_{j,i}))
$$ {#eq-collocation-spline-requirements}
for $i = 1, \dots, m$, $j = 0,1,\dots, N-1$. To put it another way, we are trying to find $N$ polynomial splines of degree $m$ to approximate the true limit cycle. As the theory of splines tells us, it is advantageous to represent a given vector polynomial $\vi u^{(j)}$ by vectors of (unknown) solutions
$$
\vi u^{j,k} = \vi u^{(j)}(s_{j,k}) \in \R^n, \; k = 0, 1, \dots, m,
$$
where $s_{j,k}$ are *representation points*^[Throughout the discussion of orthogonal collocation, we will use both *collocation* and *representation* points, which do not (in general) coincide.], i.e. nodes of equidistant partitioning of the interval
$$
s_j = s_{j,0} < s_{j,1} < \dots < s_{j,m-1} < s_{j,m} = s_{j+1},
$$
implying $\vi u^{j,m} = \vi u^{j+1,0}$, given by
$$
s_{j,k} = s_j + \frac k m (s_{j+1} - s_{k}), \; j = 0,1,\dots,N-1, k = 0,1,\dots,m.
$$
This yields the following formulation of $\vi u^{(j)}(s)$ as interpolation between values at representation points,
$$
\vi u^{(j)}(s) = \sum_{i = 0}^n \vi u^{j,i} \lagrPoly_{j,i}(s),
$$ {#eq-collocation-spline-lagrange}
where $\lagrPoly_{j,i}(s)$ are the *Lagrange basis polynomials* on $[s_j, s_{j+1}]$,
$$
\lagrPoly_{j,i}(s) = \prod_{k = 0, k \neq i}^m \frac {s - s_{j,k}} {s_{j,i} - s_{j,k}} \implies \lagrPoly_{j,i}(s_{j,k}) = \indicator{i = k} = \lcases{
	1, & i = k, \\
	0, & i \neq k.
}
$$

Using ([-@eq-collocation-spline-lagrange]), we can translate the problem of finding a vector polynomial ([-@eq-collocation-spline-requirements]) into a problem of determining the coefficients $\vi u^{j,i}$. Similarly, we can discretize the periodicity and phase conditions ([-@eq-autonomous-ode-1-periodicity-cond]), ([-@eq-integral-phase-cond]), yielding respectively
$$
\vi u^{0,0} = \vi u^{N-1, m}
$$ {#eq-collocation-discretized-periodicity}
and
$$
\sum_{j = 0}^{N-1} \sum_{i = 0}^m \omega_{j,i} \scal{\vi u^{j,i}} {\dvi v^{j,i}} = 0,
$$ {#eq-collocation-discretized-phase}
where $\dvi v^{j,i}$ are the values of the derivative of the reference period-1 solution at representation points $s_{j,i}$ and $\omega_{j,i}$ are the *Gauss-Legendre quadrature coefficients*^[Gauss-Legendre quadrature is a method of approximating a definite integral $\int_{-1}^1 f(x) \dd x \approx \sum_{i = 1}^n \omega_i f(x_i)$. The optimal value of $x_i$ and $\omega_i$ for a given value of $n$ can be computed analytically. For more information, see @Brin2020]. By combining ([-@eq-collocation-spline-requirements]), ([-@eq-collocation-spline-lagrange]), ([-@eq-collocation-discretized-periodicity]), and ([-@eq-collocation-discretized-phase]) we get a system ([-@eq-ode-discretized-bvp]) of $nmN + n + 1$ scalar equations for the unknown values $\vi u^{j,i} \in \R^n$ at representation points $s_{j,i}$ (there are $(mN + 1)$ values $\vi u^{j,i}$) and for the unknown period $T$,
$$
\rcases{
	\sum_{i = 0}^m\brackets{\vi u^{j,i}, \lagrPoly'_{j,i}(\zeta_{j,k})} - T \vi f\brackets{\sum_{i = 0}^m \vi u^{j,i} \lagrPoly_{j,i}(\zeta_{j,k})} &= \vi 0_n, \\
	j = 0, 1, \dots, N-1, \; k = 1, \dots, m, & \\
	\vi u^{0,0} - \vi u^{N-1, m} &= \vi 0_n, \\
	\sum_{j = 0}^{N-1}\sum_{i = 0}^m \omega_{j,i} \scal{\vi u^{j,i}}{\dvi v^{j,i}} &= 0.
}
$$ {#eq-ode-discretized-bvp}
The resulting finite-dimensional system ([-@eq-ode-discretized-bvp]) is nonlinear, thus we solve it by Newton's method (or some modification exploiting the block structure of Jacobian, which we will discuss later) -- this problem is well-posed by @thm-ode-correction-operator-bijection and the discussion afterward.

So far, we have translated ([-@eq-ode-cycle-bvp]) into a finite dimensional problem, but we have neglected a discussion of a crucial choice of the collocation points $\set{\zeta_{j,i}}$. The distribution of collocation points affects the approximation, so a natural goal is to minimize it. It can be shown that the optimal choice is position them at so-called *Gauss points*, which are the roots of $m$-th degree *Legendre polynomial* corresponding to the subinterval $[s_j, s_{j+1}]$. For more information, we refer the reader to @Brin2020. These roots originally belong to the interval $[-1, 1]$, but can be easily transformed to each $[s_j, s_{j+1}]$. Furthermore, the Legendre polynomials form an orthogonal system on the interval $[-1, 1]$ (or its transformation), which lends the name to the orthogonal collocation method.

Let us note the collocation at Gauss point leads to very highly accurate approximation of a smooth solution of ([-@eq-ode-cycle-bvp]) by ([-@eq-ode-discretized-bvp]) at *mesh* points, namely
$$
\norm{\vi u(s_j) - \vi u^{j, 0}} = O(h^{2m}),
$$
where $h = \max_{0 \leq j \leq N-1} (s_{j+1} - s_j)$.

Let us turn our attention back to the discretized BVP system ([-@eq-ode-discretized-bvp]) and consider its continuation variant (i.e. the discretization of ([-@eq-ode-continuation-bvp])), which can be written as $\vi H(\vi X) = \vi 0$ for appropriate $\vi H$ and $\vi X = (\set{\vi u^{j,k}}, T, \alpha) \in \R^{mnN + n + 2}$ corresponding to discretized $\vi u$, period $T$ and the continuation parameter $\alpha$. Its Jacobian matrix $\jacobi \vi H$ reads (for $n = 2$, $N = 3$, and $m = 2$)
$$
\mtr{
	\vi u^{0,0} & & \vi u^{0,1} & & \vi u^{1,0} & & \vi u^{1,1} & & \vi u^{2,0} & & \vi u^{2,1} & & \vi u^{3, 0} & & T & \alpha \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & & & & & & & & & & & \nonzeroEl & \nonzeroEl & & \\
	\nonzeroEl & \nonzeroEl & & & & & & & & & & & \nonzeroEl & \nonzeroEl & & \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & \\
}.
$$
Thus $\jacobi \vi H$ is sparse, and because it corresponds to the linear operator ([-@eq-ode-continuation-operator]) it also has a one-dimensional null-space satisfying $\vi H(\vi X) = 0$ (at generic points).

Assume that we have corrected $\vi X = (\set{\vi u^{j,k}}, T, \alpha)$, converging to a new point
$$
\vi X_c = (\set{\vi u_c^{j,k}}, T_c, \alpha_c)
$$
on the branch of the limit cycle. Then $\vi X_c$ approximates a solution to ([-@eq-ode-continuation-bvp]).

<!-- See @Govaerts2005, @Doedel1991 (same as @Kuznetsov2016) for matlab, @Verheyden2005 (or even @Lust1998) and @Roose2007 for DDEs and @Fairgrieve1991  -->
<!-- For ODEs, we can exploit the sparse structure in a direct solver (first by condensation of parameters) to obtain Floquet multipliers more easily -->
<!-- We can augment ([-@eq-ode-continuation-bvp]) by continuation method equation (Moore-Penrose or pseudo-arc-length) to get a fully-determined system -->