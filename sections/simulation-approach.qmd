---
engine: julia
---

:::{.content-visible when-format="pdf"}
\HeaderNumbered
:::


:::{.hidden}
{{< include mathematics.qmd >}}

```{julia}
#| echo: false
#| output: false

using Pkg
Pkg.activate(".")
```
:::

:::{.content-hidden unless-format="html"}
```{julia}
#| echo: false
#| output: false

using WGLMakie
WGLMakie.activate!()
```
:::

:::{.content-hidden when-format="html"}
```{julia}
#| echo: false
#| output: false

using CairoMakie
CairoMakie.activate!(type = "svg")
```
:::

# Simulation-based Approach {#sec-simulation-approach}

## Problem Formulation {#sec-problem-formulation}

Neuroscience is among the richest applications of mathematics in the current day and age. In neuronal dynamics, synchronization phenomena occurring in networks of individual neurons is of particular importance. These complex patters have been tied to the very fundamental capabilities of the brain, be it information processing, memory encoding, or the generation of rhythmic brain activity, see @Izhikevich2006, @Song2018. 

Although it is likely safe to assume a certain level of robustness of brain functions in regards to anomalies in the synchronization pattern, significant abnormalities in the rhythmic brain activity can be attributed to neurological disorders. The sheer variety of neurological disorders invalidates any sort of comprehensive study in a mere master's thesis, thus we shall only limit ourselves only to focal epilepsy. Let us note that it is still a vast topic in both neurology and applied mathematics and we will only manage to scratch the surface.

From the perspective of neuronal dynamics, these abnormalities (in the case of focal epilepsy) manifest themselves as anomalous synchronization within the neuronal network and can lead to spontaneous seizures, see @Jiruska2013. This type of synchronization is often closely linked to the presence of fast ripples (oscillations in the frequency range 250--600 Hz), very-fast ripples (600 - 1000 Hz), ultra-fast ripples (1000--2000 Hz) and ultra-fast oscillations (UFOs, above 2000 Hz) within depth electroencephalographic (EEG) recordings from patients with focal epilepsy. This possible connection between focal points of drug-resistant epilepsy and high-frequency EEG reading in their vicinity has been the cause of significant research interest due to the potential use as biomarkers for epileptogenic zones, see @Jacobs2008, @Jacobs2012, @Worrell2011, @Staba2011, @Brazdil2017, @Cimbalnik2018, @Cimbalnik2020, and @Brazdil2023. Let us note the exact mechanism governing these high frequency oscillations in EEG signals remains unknown, when physiological limitations of action potential firing rates are taken into account, see @Jiruska2017.

Conducted research sheds light on the role of anti-phase synchronization in the mechanism for the emergence of HFOs and VFHOs in epileptic EEG signals, see @Pribylova2024, @Sevcik2024, and @Zathurecky2025. The applied part of this thesis is mainly based on @Zathurecky2025 (the author of this thesis also contributed to the publication), although our focus will be the comparison between bifurcation- and simulation-based approaches to the same problem.

Let us thus consider two weakly coupled oscillators. Most commonly, these will be models of neuron connected by a (possibly delayed) gap-junctional coupling, see TODO. 
<!-- TODO: Ref mathematical neuroscience chapter -->
By phase synchronization we understand two (or more) cyclic signals featuring a consistent relationship between their phases. Note that the amplitudes or even frequencies may differs. Understanding of this phenomenon, often called *phase locking* in the context of bifurcation theory, can be aided by examining corresponding *Arnold tongues*, which are regions where a phase synchronization occurs, see @Wiggins2003, @Kuznetsov2023. 

We shall be primarily interested in a system consisting of 2 weakly coupled oscillators with possible delay in the coupling. In such systems, regions of phase-synchrony between the two oscillators may arise for certain values of the coupling strength and some parameter, which influences the oscillation frequency of one of the oscillators. Then, by the continuous dependence of the solution on parameters and initial values for both the ODEs, see @Chicone2006 (Theorem 1.3), and DDEs, see @Hale1993 (Chapter 2.2), we can explore the phase-shift in the coupled system via small changes in both parameters.

In general, the system of interest takes the following DDE form
$$
\begin{aligned}
	\dvi x_1(t) &= \vi f(\vi x_1(t); \omega_1) + \lmbd \vi K(\vi x_1(t), \vi x_2(t - \tau), \sigma), \\
	\dvi x_2(t) &= \vi f(\vi x_2(t); \omega_2) + \lmbd \vi K(\vi x_1(t - \tau), \vi x_2(t), \sigma),
\end{aligned}
$$ {#eq-general-delayed-coupled-oscillators}
where $\vi x_1(t), \vi x_2(t) \in \R^n$ are the state variables, $\omega_1, \omega_2 \in \R$ parameters influencing oscillation frequencies of the respective models, $\sigma$ is an arbitrary fixed phase parameter, and $\lmbd \in \R^+_0$ the coupling strength. Lastly, $\vi K$ represents the coupling function. Note that $\vi x_i$ does not have to represent a single neuron (or model thereof), but even synchronized clusters of neurons or neural masses^[Neural mass models approximate the behavior of large clusters of neurons based on statistically-reasoned simplifications.].

## Grid-Based Phase-Shift Computation

<!-- TODO: Use the "GridWalker" package -->
<!-- TODO: Add reference to NeuronToolbox.jl if applicable -->
<!-- TODO: Add reference to interneuron discussion and coupling discussions -->

Throughout this section, we shall iteratively develop a framework for computation of the *phase-shift* on the anti-phase tongue for 2 weakly coupled oscillators, most often neurons, their synchronized clusters, or neural masses. Thus consider the system ([-@eq-general-delayed-coupled-oscillators]) and choose $\omega_1$ or $\omega_2$ as the free parameter^[This choice is without loss of generality in case $\vi K(\cdot, \cdot)$ is symmetric.]. For illustration purposes, we shall use 2 interneuron models, see TODO, with delayed gap-junctional coupling, e.g., $\tau = 0.01$, and treat $\omega_2$ as the free parameter. Moreover, assume bounds $\omega_- \leq \omega_2 \leq \omega_+$ and corresponding discretization
$$
\underbrace{\overbrace{\omega_2^0}^{\omega_-} < \omega_2^1 < \dots < \overbrace{\omega_2^{N_{\Omega}}}^{\omega_+}}_{\Omega},
$$
with analogous bounds and discretization chosen for $\lambda$ as well, producing set $\Lambda$ of permissible values for $\lmbd$. Both are most often chosen as equidistant partitions of intervals stemming from respective bounds. We shall denote $\meshGrid = \Omega \times \Lambda$ the mesh of all possible combinations of $\omega_2^i$ and $\lmbd^i$ and let 
$$
\indicesOf{\meshGrid} = \set{\vi i = (i_1, i_2) \in \N_0^2 \divider 0 \leq i_1 \leq N_{\Omega}, 0 \leq i_2 \leq N_{\Lambda}}
$$
be the set of all possible indices in $\meshGrid$.

The most straightforward way to determine for which pairs $(\omega_2, \lmbd) \in \meshGrid$ the coupled system ([-@eq-general-delayed-coupled-oscillators]) features a *stable*^[Note that this method is effectively unable to compute unstable cycle manifolds due to the methodology itself.] anti-phase cycle is to solve the ([-@eq-general-delayed-coupled-oscillators]) forward in time from a fixed initial condition $\vi \phi \in \contStateSpace$ on a fixed time interval $[0, t_{\mathrm{end}}]$. This is a reasonable methodology if assume the system converges to the synchronized cycle and that we chose long-enough transient, included in the interval $[0, t_{\mathrm{end}}]$, such that we are in an acceptably small neighborhood around the limit cycle.

In case there is no delay in the coupling, i.e. $\tau = 0$, then we can integrate the system forward in time, see @sec-integrating-odes. Similarly, for delayed coupling one can employ the method of steps, which allows for an analogous forward-in-time integration, see @sec-integrating-ddes. Either way, this results in 2 trajectories, both corresponding to evaluations at times $\Seqnc {t^i} {i = 0} N$, namely $\Seqnc {\vi x_1^i} {i = 0} N$ and $\Seqnc {\vi x_2^i} {i = 0} N$, each for one oscillator. Moreover, denote by
$$
\Seqnc {\vi \xi^i} {i = 1} N = \Seqnc {\mtr{\vi x_1^i \\ \vi x_2^i}} {i = 0} N
$$
the joint trajectory. For simplicity, we shall assume that the time interval is divided equidistantly, i.e. there is a constant time-step $\diff t$.

Now, we face 2 main challenges:

1. How to determine the period $T$ of the coupled system ([-@eq-general-delayed-coupled-oscillators])?
2. Knowing the period $T$, how to calculate the phase-shift $\beta$ between the oscillators?

Let us note all the following numerical experiments and calculations were done in Julia using a custom-made package [`GridWalker.jl`](https://gitlab.com/sceptri-university/gamu/gridwalker.jl).

### Period Searching {#sec-period-searching}

<!-- TODO: Change @Pribylova2025 from SSRN to the actual publication -->
To calculate the joint period $T$ of ([-@eq-general-delayed-coupled-oscillators]) from $\Seqnc {\vi \xi^i} {i = 0} N$, we propose two distinct methods. The first method relies on the fact that for capacitance-based models of neurons, there are certain notable components, e.g., the membrane potential, which should predominantly determine the overall behavior of the neuronal model. Moreover, we assume the periodic signal of this state variable behaves rather nicely, attaining single (local) minimum and maximum on the repeating section of minimal period. As a counterexample to this assumption, the bursting behavior of pyramid cells might be considered (in such case, an envelope would have to be constructed in order to proceed as described), see @Pribylova2025. By choosing the $j$-th element of the state vector corresponding to the coupled model, we get a 1-dimensional time series $\Seqnc {\xi^i_j} {i = 0} N$. One can then compute first differences $\Seqnc {\diff \xi^i_j} {i = 1} N$, where $\diff \xi^i_j \letDef \xi^i_j - \xi^{i - 1}_j$.

Clearly, (local) extrema of the time series $\Seqnc {\xi^i_j} {i = 0} N$ occur when the differences $\Seqnc {\diff \xi^i_j} {i = 1} N$ change sign^[The order determines whether its a local minimum or maximum.]. In other words, we attempt to detect peaks (which should reflect the true period) as indices satisfying
$$
i \text{ is a peak } \iff \diff \xi^i_j > 0 \; \land \; \diff \xi^{i+1}_j < 0.
$$
Let us denote $I^{\uparrow}_j$ the set of all peaks originating from the choice of notable index $j$, then the period $T$ can be approximated as the $\diff t$ multiple of the *mean difference* $\avg{\diff I^{\uparrow}_j}$ of the peak indices, i.e.
$$
T \approx \estim T_{\diff} = \diff t \cdot \avg{\diff I^{\uparrow}_j}.
$$
We shall call $\estim T_{\diff}$ the *differences period estimate*. Note that while this method is rather inexpensive from the computational point of view, it depends on the appropriate choice of $j$ and the necessary assumption that the period of $\xi_j$ accurately reflects the period of the joint oscillation. For an example, see @fig-diff-period.

::: {.callout-tip #tip-monotony-extension}
##### On the length of monotony of $\xi^i_j$

Currently, we require the $\xi^i_j$ to be strictly increasing only one step left of a peak and decreasing also for only one step right of the peak. This can be extended to require $\xi^i_j$ to be increasing for $k_+$ steps on the left and decreasing for $k_-$ steps on the right of a peak. Such modification allows us to detect periods of much more complicated signals.
:::


```{julia}
#| label: fig-diff-period
#| fig-cap: Approximation of the period of a test function $y(t)= \sin(t) + \frac {\cos\brackets{2t + \frac {\pi} 3}} 3$ (with true period $2\pi$) by *differences period* estimation method.
#| width: 70%
using LaTeXStrings, Statistics

fig = Figure(size=(450, 250))
ax1 = Axis(fig[1, 1], yticksvisible=false, yticklabelsvisible=false, ylabel=L"y(t)")
ax2 = Axis(fig[2, 1], yticksvisible=false, yticklabelsvisible=false, ylabel=L"\Delta y(t)")

linkxaxes!(ax1, ax2)


Δt = 0.5

t = 0:Δt:13π;
n = length(t);
y = @. sin(t) + 1 / 3 * cos(2 * t + π / 3)
ts = t[2:end]
dy = diff(y)

dy_up = dy .> 0
dy_down = dy .< 0

# Find where the monotony changes from up to down
y_monotony = dy_up[2:end] .&& dy_down[1:end-1]

# Compute mean distance between ups
Iⱼ = findall(identity, y_monotony)

for i in Iⱼ
    vlines!(ax1, ts[i]; linewidth=2, color=ones(n), colormap=:Zissou1)
    vlines!(ax2, ts[i]; linewidth=2, color=ones(n), colormap=:Zissou1)
end
scatterlines!(ax1, t, y, colormap=:Zissou1)
scatterlines!(ax2, ts, dy, colormap=:Zissou1, color=[el > 0 ? 1 : 2 for el in dy])

ΔĪⱼ = mean(diff(Iⱼ))
T̂ = ΔĪⱼ * Δt

ax2.xlabel = LaTeXString("\$\\hat{T}_{\\Delta} \\approx $T̂\$")

fig
```

The second approach one can use is less heuristic, but potentially more computationally intensive. Naturally, the joint period $T$ should minimize the discrepancy between $t$ and $t + T$ states, i.e.,^[We are interested in the *minimal period*, thus we add $\min$ to select the least of all permissible values in ([-@eq-period-minimization-problem]).]
$$
T = \minOf{\argmin_{T > 0} \int_a^b \norm{\vi \xi(t) - \vi \xi(t + T)} \dd t}, \quad (a,b) \subset \R.
$$ {#eq-period-minimization-problem}

As it stands, ([-@eq-period-minimization-problem]) is infeasible to solve computationally, but one can rather easily discretize it using $\Seqnc {\vi \xi^i} {i = 0} N$ to obtain
$$
\estim T_{\min, P} = \argmin_{T \in P} \overbrace{\sum_{i = 0}^{i_{\mathrm{end} - T}} \norm{\vi \xi^i - \vi \xi^{i + \floor{T/\diff t}}}}^{f_{\vi \xi}(T)},
$$ {#eq-discretized-period-minimization}
where $P = (p_-, p_+)$, with $p_+ > p_- > 0$, is a chosen interval and $i_{\mathrm{end} - T}$ denotes the maximum index $i$ such that $t^i \leq t_{\mathrm{end}} - T$. The discretized problem ([-@eq-discretized-period-minimization]) can now be solved using discussed optimization methods for one-dimensional optimization, see @sec-1d-optimization, under the assumption of *unimodality*!^[Without unimodality, these methods converge to a local minimum near the starting point.]

Note that $\estim T_{\min, P}$ is *not* guaranteed to approximate the minimal period. In practice, we often repeat solving ([-@eq-discretized-period-minimization]) for multiple values of $P_i$ and choose the estimate $\estim T_{\min, P_i}$ with the least error. In particular, we found that using $P_i = \brackets{p_-, \frac {p_+}{2^{i-1}}}$ gives satisfactory results. Moreover, for short interval $P$, unimodality of the loss function corresponding to ([-@eq-discretized-period-minimization]) is likely to be satisfied. In addition, @fig-optim-period provides an illustration of the algorithm.

```{julia}
#| label: fig-optim-period
#| fig-cap: Approximation of the period of a test function $y(t)= \sin(t) + \frac {\cos\brackets{2t + \frac {\pi} 3}} 3$ (with true period $2\pi$) by *optimal period* estimation method.
#| width: 70%
using Optim, LinearAlgebra

fig = Figure(size=(450, 250))
ax1 = Axis(fig[1, 1], yticksvisible=false, yticklabelsvisible=false, ylabel=L"y(t)")
ax2 = Axis(fig[2, 1], yticksvisible=false, yticklabelsvisible=false, ylabel=L"f_{\mathbf{\xi}}(T)")

P = (1, 10)
xlims!(ax2, P)

Δt = 0.5
t = 0:Δt:13π;
n = length(t);
y = @. sin(t) + 1 / 3 * cos(2 * t + π / 3)

i_end(T) = findlast(tᵢ <= (t[end] - T) for tᵢ in t)

T_ex = 3.2
f_ξ(T) = sum(norm(y[i] - y[i+floor(Int, T / Δt)]) for i in 1:i_end(T))
p = first(P):0.01:last(P)

result = Optim.optimize(f_ξ, P..., Optim.GoldenSection())
T̂ = round(result.minimizer, digits=2)

ax2.xlabel = LaTeXString("\$\\hat{T}_{\\min, P} \\approx $T̂\$")

scatterlines!(ax1, t[1:i_end(T_ex)], [y[i+floor(Int, T_ex / Δt)] for i in 1:i_end(T_ex)], color=(:gray, 0.4), markersize=5)
scatterlines!(ax1, t[1:i_end(T̂)], [y[i+floor(Int, T̂ / Δt)] for i in 1:i_end(T̂)], color=ones(i_end(T̂)), colormap=:Zissou1, markersize=5)
scatterlines!(ax1, t, y, colormap=:Zissou1, markersize=5)

vlines!(ax2, T_ex, color=(:gray, 0.4))
vlines!(ax2, T̂, color=[1], colormap=:Zissou1)
lines!(ax2, p, f_ξ.(p); colormap=:Zissou1)
scatter!(ax2, [T_ex], [f_ξ(T_ex)], color=:gray)
scatter!(ax2, [T̂], [f_ξ(T̂)], color=[1], colormap=:Zissou1)

fig
```

::: {.callout-tip #tip-optimal-period-modifications}
##### Modifications to the optimal period method

In ([-@eq-discretized-period-minimization]), we have used the full state vector for discrepancy calculation, but similarly as with the computation of $\estim T_{\diff}$, one can choose only a sub-vector as well. Another common modification is to calculate the dissimilarity in ([-@eq-discretized-period-minimization]) after a certain transient has passed (such that we presume we have converged to the cycle after the transient), i.e., on interval with indices from $i_{\mathrm{start}}$ to $i_{\mathrm{end} - T}$.
:::

### Shift Searching {#sec-shift-searching}

After one has successfully estimated the period in the joint model, what remains is to approximate the shift between the two oscillators of ([-@eq-general-delayed-coupled-oscillators]). This can be achieved by solving a very similar problem ([-@eq-discretized-period-minimization]), namely
$$
\estim \beta_d = \argmin_{\beta \in Q} \sum_{i = i_{\mathrm{start}}}^{i_{\mathrm{end} - \beta}} d\brackets{\vi x_1^i, \vi x_2^{i + \floor{\beta/\diff t}}},
$$ {#eq-discretized-shift-minimization}
where $Q = (q_-, q_+)$ is a chosen interval with $0 \leq q_- < q_+ < \estim T$ and $d$ is a metric. Commonly, metrics induced by $\ltwo$-norm $\norm{\cdot}_2$ or maximum-norm $\norm{\cdot}_{\infty}$ are used. Moreover, $i_{\mathrm{start}}$ can again be used to offset the computation of shift $\beta$ after a transient has passed.

Note that we shall primarily focus on calculating a shift $\beta$ between periodic trajectories of a notable component $j$ in each of the oscillators, i.e.,
$$
\estim \beta_{d,j} = \argmin_{\beta \in Q} \sum_{i = i_{\mathrm{start}}}^{i_{\mathrm{end} - \beta}} d\brackets{x_{1,j}^i, x_{2,j}^{i + \floor{\beta/\diff t}}}.
$$ {#eq-1d-discretized-shift-minimization}
In total, ([-@eq-discretized-shift-minimization]) and ([-@eq-1d-discretized-shift-minimization]) both lead to an one-dimensional optimization problem on a given interval, and as such can be solved using methods proposed in @sec-1d-optimization. In particular, in the case of shift searching, unimodality of the corresponding loss function seems to be satisfied more generally.

For comparison of computed shifts in regards to period estimation based on differences and optimization, see @fig-cold-diff-shift and @fig-cold-diff-shift-delay. There it is apparent the central Arnold tongue has a rather sharp boundary at some places, which is desired from the theoretical point of view, see @Kuznetsov2023 and @Zathurecky2025, as it is known that Arnold tongues are bordered by limit point of cycle curves. On the other hand, for both low and high values of $\lambda$, our approximation of the tongue is either smoothed out (likely signalling that our choice of the initial condition was suboptimal) or scattered.

::: {.callout-note #nte-arnold-tongues}
##### Arnold tongue

Arnold tongues are, in general, regions (in parameter space) of rational^[By *rational synchrony* we mean phase-locked synchronization in the form $p:q$, $p,q \in \N$ indivisible, i.e., one period consists of $p$ revolutions around the first axis and $q$ revolutions around the second. For two weakly coupled oscillators, this translates to a period of the full system being completed after $p$ periods of the first oscillators and $q$ of the second one.] synchrony near a Neimark-Sacker bifurcation. For a continuous-time dynamical system, this, in essence, means that a given cycle switches stability and a torus with the opposite stability emerges around it (i.e., a stable cycle loses stability when it undergoes the Neimark-Sacker bifurcation while a stable torus appears around the now-unstable cycle). The dynamics on torus then provide the two "directions of oscillation", for which we can study its synchrony. Furthermore, two weakly coupled oscillators themselves are governed by dynamics on a torus, such that influence of coupling strength $\lmbd$ mimics the transition normally originating from the Neimark-Sacker bifurcation.
:::

```{julia}
#| echo: false
#| output: false
using ColorSchemes, JLD2
CyclicZissou = ColorScheme([ColorSchemes.Zissou1.colors..., reverse(ColorSchemes.Zissou1.colors)...]);
```

::: {#fig-cold-diff-shift layout-ncol="2"}
```{julia}
#| echo: false
#| label: fig-cold-diff-shift-a
#| fig-cap: "*Differences* period estimation"
jldopen("../data/no_delay_cold_diff_no_checker.jld2", "r") do data
	fig = Figure(size=(600, 400))
	ax = CairoMakie.Axis(fig[1, 1];
		xlabel=L"C_2", ylabel=L"\lambda")

	hmap = heatmap!(ax,
		range(data["C₂"]...),
		range(data["λ"]...),
		transpose(data["shifts"]), colormap=CyclicZissou)
	Colorbar(fig[1, 2], hmap; label=L"\beta", width=15, ticksize=15, tickalign=1)

	fig
end
```

```{julia}
#| echo: false
#| label: fig-cold-diff-shift-b
#| fig-cap: "*Optimal* period estimation"
jldopen("../data/no_delay_cold_optimal_no_checker_among_halves.jld2", "r") do data
	fig = Figure(size=(600, 400))
	ax = CairoMakie.Axis(fig[1, 1];
		xlabel=L"C_2", ylabel=L"\lambda")

	hmap = heatmap!(ax,
		range(data["C₂"]...),
		range(data["λ"]...),
		transpose(data["shifts"]), colormap=CyclicZissou)
	Colorbar(fig[1, 2], hmap; label=L"\beta", width=15, ticksize=15, tickalign=1)

	fig
end
```

Heatmap of shifts $\beta$ between two $\lmbd$-weakly coupled interneuron models with respect to $C_2$ without delay. Grid is determined as 75-points discretizations of intervals of interest for both $C_2$ and $\lambda$. At every point the same initial condition $\vi \phi(\cdot) \equiv \vi u_0$ was used. Left figure presents the shift heatmap using *differences* period, whereas the right figure shows *optimal* period-based shift. Trajectories are computed for 1500 ms, periods are computed on the last 20% and shifts on the last 10% of their lengths. Note that optimal period is calculated by halving the upper constraint up-to 4 times. Shift is estimated based on the first state variable, i.e. $V$, in both interneurons.

:::

::: {#fig-cold-diff-shift-delay layout-ncol="2"}
```{julia}
#| echo: false
#| label: fig-cold-diff-shift-delay-a
#| fig-cap: "*Differences* period estimation"
jldopen("../data/delay_cold_diff_no_checker.jld2", "r") do data
	fig = Figure(size=(600, 400))
	ax = CairoMakie.Axis(fig[1, 1];
		xlabel=L"C_2", ylabel=L"\lambda")

	hmap = heatmap!(ax,
		range(data["C₂"]...),
		range(data["λ"]...),
		transpose(data["shifts"]), colormap=CyclicZissou)
	Colorbar(fig[1, 2], hmap; label=L"\beta", width=15, ticksize=15, tickalign=1)

	fig
end
```

```{julia}
#| echo: false
#| label: fig-cold-diff-shift-delay-b
#| fig-cap: "*Optimal* period estimation"
jldopen("../data/delay_cold_optimal_no_checker_among_halves.jld2", "r") do data
	fig = Figure(size=(600, 400))
	ax = CairoMakie.Axis(fig[1, 1];
		xlabel=L"C_2", ylabel=L"\lambda")

	hmap = heatmap!(ax,
		range(data["C₂"]...),
		range(data["λ"]...),
		transpose(data["shifts"]), colormap=CyclicZissou)
	Colorbar(fig[1, 2], hmap; label=L"\beta", width=15, ticksize=15, tickalign=1)

	fig
end
```

Heatmap of shifts $\beta$ between two $\lmbd$-weakly coupled interneuron models with respect to $C_2$ with delay $\tau = 0.05$. Note that except for the time integration (performed by method of steps, see @sec-method-of-steps), the same methods were used as in @fig-cold-diff-shift.

:::

### Starters, Iterators and Indexers {#sec-starters-iterators-indexers}

#### Starters

So far, we have utilized a rather naive approach of using always the same predetermined starting condition -- we shall refer to this as a *cold* start^[For reference on the terminology, see the [cold start Wikipedia entry](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)).]. Nevertheless, we have no guarantee such choice was optimal for any grid point, much less for the entire grid. 

A natural solution to this problem is to re-use trajectories we have already computed for some other combination of parameters. In general, it is going to be beneficial to use choose as an initial condition the endpoint of the trajectory corresponding to parameter values *close* to our current ones. Moreover, we want to consider only those nearby parameter values such that their trajectories lead to anti-phase synchronization. In total, the optimal reference index $\starter(\vi i) \in \indicesOf{\meshGrid}$, from which to read the new initial condition, for the shift computation at $\vi i$ (using $\vi i \in \indicesOf{\meshGrid}$ to denote the combined index in both axis of the grid $\meshGrid$) is given by the following minimization problem,
$$
\starter_{\indicesOf{\meshGrid}}(\vi i) = \argmin_{\vi j \in \indicesOf{\meshGrid}} \loss(\vi i, \vi j, \beta_{\vi j}),
$$ {#eq-mesh-warm-start-minimization}
where we use either the *multiplicative* loss, 
$$
\loss_{\mathrm{mult}}(\vi i, \vi j, \beta) = \norm{\vi i - \vi j} \cdot \brackets{\absval{\beta - \frac 1 2} + c},
$$ {#eq-starter-multiplicative-loss}
where $c > 0$ is an offset to the discrepancy in shift from anti-phase, or the *additive* loss,
$$
\loss_{\mathrm{add}}(\vi i, \vi j, \beta) = c_1 \norm{\vi i - \vi j} + c_2 \absval{\beta - \frac 1 2},
$$ {#eq-starter-additive-loss}
where $c_1, c_2 > 0$ are chosen weights. Note that we will use additive loss ([-@eq-starter-additive-loss]) if not stated otherwise, as it seemed to perform generally better for our needs.

While this approach is ideal in theory, in practice we run into two main issues. Firstly, during the computation, some of the indices are not yet evaluated, thus we do not know their respective values of shifts $\beta_{\vi i}$. This can be remedied by defining the set of all *evaluated* indices $\IndicesOf{\meshGrid}{E}$ and restricting ([-@eq-mesh-warm-start-minimization]) to it, yielding
$$
\starter_{\IndicesOf{\meshGrid}{E}}(\vi i) = \argmin_{\vi j \in \IndicesOf{\meshGrid}{E}} \loss(\vi i, \vi j, \beta_{\vi j}).
$$ {#eq-warm-start-minimization}

#### Indexers

The second issue is present even in ([-@eq-warm-start-minimization]), and it is the dependence on the entire grid. Indeed, for distant indices the loss function, be it ([-@eq-starter-multiplicative-loss]) or ([-@eq-starter-additive-loss]), will be large and irrelevant for the minimum. Hence, we can define an *indexer* function
$$
\indexer : \indicesOf{\meshGrid} \to 2^{\indicesOf{\meshGrid}}
$$ {#eq-indexer-def}
specifying a permissible neighborhood for every point of the grid. 

Arnold tongues have a distinct shape, see @Sevcik2021, @Kuznetsov2023 or @Zathurecky2025, which we can exploit to restrict the search space of a starter. Indeed, if assume the Arnold tongue with the anti-phase synchrony is present approximately in the middle of our grid $\meshGrid$, then elements closer to the center are more likely to live inside the tongue (and therefore be a good initial guess for anti-phase synchronized trajectories for nearby parameter values). Thus, choosing a starter only among evaluated indices on the diagonal from the current index to the center of $\indicesOf{\meshGrid}$ up-to some distance away, as can be seen on the following illustration, should not worsen our initial guess as compared to the full *warm starter*.
$$
\begin{array}{cccccc}
	& & & & & \text{center of } \indicesOf{\meshGrid} \\
	& & & & \nearrow & \\
	\circ_3 & \circ_3 & \circ_3 & \circ_3 & & \\
	\circ_2 & \circ_2 & \circ_2 & \circ_3 & & \\
	\circ_1 & \circ_1 & \circ_2 & \circ_3 & & \\
	\bullet_{\vi i} & \circ_1 & \circ_2 & \circ_3 & &
\end{array}
$$
We refer to the described method as the *diagonal indexer* of length $l_{\indexer} \in \N$ (example shown with $l_{\indexer} = 3$). The combination of a *warm starter* with *indexer*, which we shall call an *indexed starter*, yields the minimization problem
$$
\starter_{\indexer_E}(\vi i) = \argmin_{\vi j \in \IndicesOf{\meshGrid}{E} \cap \indexer(\vi i)} \loss(\vi i, \vi j, \beta_{\vi j}).
$$ {#eq-warm-start-diagonal-indexer}
Depending on the choice of the indexer, an indexed starter might bring a very important computational benefit of *constant complexity* with respect to grid size. Indeed, by employing a fixed-length diagonal indexer, the indexed starter^[The naive warm starter has a time complexity $O(N_{\Omega} \cdot N_{\Lambda})$ when considering the computation of shift to have constant complexity for all indices $\vi i$.] $\starter_{\indexer_E}(\vi i)$ requires comparison of at most $l_{\indexer}^2 - 1$ values of $\loss(\vi i, \vi j, \beta_{\vi j})$, where $l_{\indexer} \ll \minOf{N_{\Omega}, N_{\Lambda}}$.

#### Iterators

So far, we have discussed how to choose initial conditions from endpoints of already computed trajectories and how to limit the number of candidate indices. The last undetermined process is the actual order of evaluation of indices. For cold starts, the order could be arbitrary, as it did not influence the rest of the algorithm. However, for *warm* ([-@eq-warm-start-minimization]) and *indexed* starters ([-@eq-warm-start-diagonal-indexer]), we have an explicit dependence on already evaluated indices and their corresponding trajectories. In both cases, if no viable indices are found the starter falls back to a fixed initial condition (i.e., to cold starter).

The "natural" order of iteration along the mesh is deduced from its representation in the Julia code as two separate ranges. Then iterating over their cartesian product, i.e., the grid $\meshGrid$, amounts to subsequently evaluating each column of $\meshGrid$, see @fig-iterators-a. Notice that such evaluation order is suboptimal for the *diagonal indexer*, which will likely default to a cold start for indices on the left side of the grid.
```{julia}
#| echo: false
#| output: false
using GridWalker
l = 10
λ = range(start=0.0, stop=0.035, length=l)
C₂ = range(start=0.95, stop=1.05, length=l)

walker_params = parametrize_walker(;
    min_x_param=1, max_x_param=1, x_param_density=1,
    min_y_param=1, max_y_param=1, y_param_density=1)

order_of_iteration(enumerator) = begin
    I𝕄 = zeros(l, l)

    k = 1
    for (i, _) in enumerator(C₂, λ, walker_params)
        I𝕄[i...] = k
        k += 1
    end

    return I𝕄
end

```

::: {#fig-iterators layout-ncol="3"}
```{julia}
#| echo: false
#| label: fig-iterators-a
#| fig-cap: "Line iterator"
hmap, ax = heatmap(C₂, λ, order_of_iteration(GridWalker.line_enumerator), colormap=:grays)
ax.xlabel = L"C_2"
ax.ylabel = L"\lambda"

hmap
```

```{julia}
#| echo: false
#| label: fig-iterators-b
#| fig-cap: "Centered spiral iterator"
hmap, ax = heatmap(C₂, λ, order_of_iteration(GridWalker.spiral_enumerator), colormap=:grays)
ax.xlabel = L"C_2"
ax.ylabel = L"\lambda"

hmap
```

```{julia}
#| echo: false
#| label: fig-iterators-c
#| fig-cap: "Shifted spiral iterator"
shifted_spiral = (args...) -> GridWalker.spiral_enumerator(args...; 
	center_function=GridWalker.relative_center,
    fixed_ratio_center=(0.4, 0.8))

hmap, ax = heatmap(C₂, λ, order_of_iteration(shifted_spiral), colormap=:grays)
ax.xlabel = L"C_2"
ax.ylabel = L"\lambda"

hmap
```

Heatmaps indicating evaluation order (from black to white) for different kinds of iterators from `GridWalker.jl`.

:::

From the perspective of stability of tongue detection, a more robust strategy is to start the evaluation order from the (presumed) center of the tongue and spirally iterate outwards. This way, an indexed starter can likely initialize from a parametrically-nearby trajectory in anti-phase synchronization, because suitable candidates are evaluated first, see @fig-iterators-b and @fig-iterators-c.

All these concepts can now be combined together. In @fig-indexed-shift, one can the same heatmaps corresponding to shift between two weakly coupled interneurons as in @fig-cold-diff-shift and @fig-cold-diff-shift-delay. In particular, an indexed starter $\starter_{\indexer}$ was used with diagonal indexer of length $l_{\indexer} = 10$ on 75-by-75 grid $\meshGrid$. An additive loss ([-@eq-starter-additive-loss]) was used for the starter in conjunction with spiral iterator from the true center. The rest of the experiment setup follows @fig-cold-diff-shift.


::: {#fig-indexed-shift layout-ncol="2"}
```{julia}
#| echo: false
#| label: fig-indexed-shift-a
#| fig-cap: "$\\tau = 0$"
jldopen("../data/no_delay_spiral_indexed_optimal_no_checker_among_halves.jld2", "r") do data
    fig = Figure(size=(600, 400))
    ax = CairoMakie.Axis(fig[1, 1];
        xlabel=L"C_2", ylabel=L"\lambda")

    hmap = heatmap!(ax,
        range(data["C₂"]...),
        range(data["λ"]...),
        transpose(data["shifts"]), colormap=CyclicZissou)
    Colorbar(fig[1, 2], hmap; label=L"\beta", width=15, ticksize=15, tickalign=1)

    fig
end
```

```{julia}
#| echo: false
#| label: fig-indexed-shift-b
#| fig-cap: "$\\tau = 0.025$"
jldopen("../data/delay_spiral_indexed_optimal_no_checker_among_halves.jld2", "r") do data
    fig = Figure(size=(600, 400))
    ax = CairoMakie.Axis(fig[1, 1];
        xlabel=L"C_2", ylabel=L"\lambda")

    hmap = heatmap!(ax,
        range(data["C₂"]...),
        range(data["λ"]...),
        transpose(data["shifts"]), colormap=CyclicZissou)
    Colorbar(fig[1, 2], hmap; label=L"\beta", width=15, ticksize=15, tickalign=1)

    fig
end
```

Heatmap of shifts $\beta$ between two $\lmbd$-weakly coupled interneuron models with respect to $C_2$ with and without delay. The heatmaps are computed by utilizing introduced techniques of starters, indexers and iterators.
:::

Notice that in @fig-indexed-shift-a, the boundary of the tongue for higher coupling strengths is sharper than in @fig-cold-diff-shift. Also, sub-tongues present in all previous results now appear thinner and more distorted.

### Periodicity Checkers

In Figures [-@fig-cold-diff-shift] to [-@fig-indexed-shift], we have always computed a trajectory for each parameter combination belonging to a point on the grid $\meshGrid$ and then attempted to find corresponding period and shift. This relied on the assumption that the (joint) trajectory is always periodic, and thus all necessary quantities are well defined and can be computed. In case the assumption does not hold, we will still get an output, but potentially a nonsensical one.

In other words, one should check for periodicity of the trajectory for each parameter combination. If periodicity is not confirmed, we can skip the rest of the algorithm, saving valuable compute time.

There are several ways one might go about 