---
engine: julia
---

:::{.content-visible when-format="pdf"}
\HeaderNumbered
:::


:::{.hidden}
{{< include mathematics.qmd >}}

```{julia}
#| echo: false
#| output: false

using Pkg
Pkg.activate(".")
```
:::

:::{.content-hidden unless-format="html"}
```{julia}
#| echo: false
#| output: false

using WGLMakie
WGLMakie.activate!()
```
:::

:::{.content-hidden when-format="html"}
```{julia}
#| echo: false
#| output: false

using CairoMakie
CairoMakie.activate!()
```
:::

# Bifurcation Theory Approach {#sec-bifurcation-approach}

As we have mentioned several times before, the fundamental goal of this thesis is to study the computational aspects of continuation/detection of stable and unstable manifolds of periodic orbits (and what challenges come with it). As such, we shall properly discuss what a continuation is and how we compute it. We begin with a theoretical treatment of bifurcation theory, focused solely on cycle continuation and stability identification, followed by original results concerning coupled models of neurons. The theoretical introduction is mainly based on @Kuznetsov2023, @Guo2013, and @Smith2010, but many more related articles are cited throughout the discussion.

> We should also disclose that while this chapter is named 'Bifurcation Theory Approach', we will spend relatively little time discussing actual bifurcations. Our main focus lies in the continuation of (un)stable cycles.

<!-- TODO: Change the title -->
## Theory of Localizations

Although the primary subjects of our study are periodic orbits, we will first delve into the localization of equilibrium. Indeed, one might recall we discussed the relation of equilibria (or fixed points) in discrete dynamical systems and periodic orbits of continuous-time dynamical systems in @sec-poincare-map. This fact alone suggests it is beneficial to first study the simpler case of equilibria localization, building tools to later tackle the periodic case.

### Dynamical Systems

#### Equilibrium Localization

To start relatively simple, consider a continuous-time dynamical system induced by an autonomous ODE of form ([-@eq-autonomous-ode-system]), i.e.,
$$
\dvi x = \vi f(\vi x), \; \vi x \in \R^n.
$$
Its equilibrium $\vi x^*$ is then given by
$$
\vi f(\vi x^*) = 0.
$$ {#eq-equilibrium-cond}
Note that similarly by setting $\vi f(\vi x) = \vi g(\vi x) - \vi x$ we can get the same equilibrium condition for a discrete-time dynamical system $\vi x \mapsto \vi g(\vi x)$.

It is easy to see that if the equilibrium $\vi x^*$ is stable, one can integrate the system ([-@eq-autonomous-ode-system]) forward in time starting from some point $\vi x$ in the basis of attraction of $\vi x^*$, since
$$
\lim_{t \to \infty} \norm{\vi \evolOp(t, \vi x) - \vi x^*} = 0
$$
by @def-invariant-set-stable. Similarly, if $\vi x^*$ is *totally unstable* (i.e., *repelling* from all directions), we can simply reverse time (as we are in the *ODE* case) and repeat the procedure^[Note that this behavior is *local* and global properties are often more complicated, which is even more prevalent in higher dimensions.].

Nonetheless, in general, this approach will not work, as equilibria are rarely stable. The standard procedure is to start from a point in a small neighborhood around the equilibrium (determined either analytically or via numerical integration) and construct a sequence of point $\Seqnc {\vi x^{(i)}} {i = 0} \infty$, which converges to $\vi x^*$ under general conditions. For this purpose, we can use the Newton's method, see @sec-newton-method, or one of its modifications.

Suppose we have found $\vi x^*$ with desired accuracy. Our next task is then to determine its stability, i.e., how the phase portrait behaves in its vicinity. By @thm-lyapunov-stability-theorem, @thm-grobman-hartman-cont and the following discussion, we know that eigenvalues of the Jacobian matrix $\Jacobi^*$ determine the stability of $\vi x^*$. In other words, we need to compute the roots of the *characteristic polynomial*
$$
p(\lmbd) = \det (\Jacobi^* - \lmbd \ident).
$$
Note that there are other methods of computing stability, which in particular do not require eigenvalues, but only computation of certain determinants of $\Jacobi^*$. We refer the interested reader to @Kuznetsov2023, page 470.

#### Periodic Orbit Localization {#sec-ode-periodic-orbit-localization}

As we have seen, localization of an equilibrium from sufficiently close initial guess was rather simple both theoretically and computationally. Unfortunately, for limit cycles the situation is more complicated. For clarity, @Kuznetsov2016 was heavily used as a source material for this section.

Assume again we have a dynamical system prescribed by ([-@eq-autonomous-ode-system]), 
$$
\dvi x = \vi f(\vi x), \; \vi x \in \R^n,
$$
such that is has an isolated periodic orbit $L_0$. Let $\vi x^0(t + T_0) = \vi x^0(t)$ be the corresponding solution with minimal period $T_0 > 0$. Further assume that the cycle $L_0$ has corresponding multipliers, see @sec-poincare-map and @lem-poincar√©-map, 
$$
\mu_1, \dots, \mu_n \in \C,
$$
which are by @thm-floquet-exponents the eigenvalues of the $n\times n$ monodromy matrix $\vi M(T_0)$, where $\vi M(t)$ satisfies
$$
\rcases{
	\dvi M(t) &= \jacobi \vi f (t) \vi M(t), \\
	\vi M(0) &= \ident_n.
}
$$ {#eq-ode-monodromy-system}
Recall there is always a trivial multiplier $\mu_1 = 1$. Also, if $\absval{\mu_i} < 1$ for all $i \in \oneToN{n}$, then the cycle is stable. In contrast, if there exists $i \in \oneToN{n}$ such that $\absval{\mu_i} > 1$, the cycle is unstable (and if all multipliers have modulus greater than one, the cycle is called *totally unstable* or *repelling*).

If the system features a stable limit cycle $L_0$, then we can find it again by numerical integration from a point in its basin of attraction. In contrast to the case of equilibria, backward-time numerical integration will not, in general, help us find a repelling periodic orbit. 

Suppose we have a system in *2-dimensional* state space, which possesses a periodic orbit, see @fig-2d-periodic-orbit. From the theory of differential equations, we know that in the case of an autonomous ODE ([-@eq-autonomous-ode-system]) its trajectories do not intersect. Thus a periodic orbit $L_0$ constitutes a Jordan curve^[By ([-@eq-autonomous-ode-system]) and the periodicity constraint, there exists a map $\vi \phi(t) = \evolOp(t, \vi x(t))$ such that it maps the interval $[t_0, t_0 + T_0]$ onto the cycle $L_0$ and is surely continuous by @def-dynamical-system and @def-autonomous-ode-system for sufficiently smooth $\vi f$ (otherwise $\vi f$ would not give rise to a dynamical system in the first place). Moreover, $\vi \phi(t)$ is injective on $[t_0, t_0 + T_0)$ as there can be no intersections. In total, $L_0 = \vi \phi([t_0, t_0 + T_0])$ is a Jordan curve.]. Then by Jordan curve theorem, see @wikiJordanCurve, such orbit partitions the state space into inside and outside of the cycle (and the cycle itself)^[In practice, it suffices to consider a sufficiently large connected region containing the cycle instead of the entire state space]. Thus if $L_0$ is repelling, trajectories are guaranteed to converge to $L_0$ backward in time.

![Unstable periodic orbit $L_0$ of a 2-dimensional dynamical system.](../diagrams/2D-state-space-periodic-orbit.drawio.svg){#fig-2d-periodic-orbit .final}

On the other hand, for 3- or more dimensional dynamical systems, a periodic orbit does not partition the state space (or any local neighborhood). Then, we cannot guarantee a convergence to the repelling cycle by backward-time integration no matter the initial vicinity. Moreover, it is needless to say the numerical integration localization approach will surely fail if the cycle is not stable nor repelling.

From now on, we will assume we know the location of the periodic orbit approximately. A correction method, most often Newton-Raphson, is then applied repeatedly until satisfactory convergence. This is a rather common scenario in practice, where we often know the location of the cycle for a given parameter value and wish to "continue" by varying the parameter and correcting the cycle afterwards. This process is commonly referred to as *continuation*.

Usually, we take the period $T$ of $L_0$ as an unknown and formulate the *boundary-value problem* (BVP) on a fixed interval. In particular, assume $T$ is a parameter and construct a "time-normalized" system
$$
\deriv {\vi u} {s} = T \vi f(\vi u), \; s \in [0,1],
$$ {#eq-autonomous-ode-period-1}
which rescales ([-@eq-autonomous-ode-system]) by a time-scaling factor $T$, such that the new time is denoted $s$. Now, if a solution $\vi u(s)$ to ([-@eq-autonomous-ode-period-1]) also satisfies the *periodic boundary conditions*
$$
\vi u(0) = \vi u(0),
$$ {#eq-autonomous-ode-1-periodicity-cond}
then it corresponds to a $T$-periodic solution of ([-@eq-autonomous-ode-system]). However, these two conditions do not prescribe the period orbit of ([-@eq-autonomous-ode-system]) uniquely. Indeed, any time shift of the solution to the BVP ([-@eq-autonomous-ode-period-1]), ([-@eq-autonomous-ode-1-periodicity-cond]) also determines the same periodic orbit.

Therefore, an extra condition, called the *phase condition*, must be added. Such condition is most often written in the form
$$
\Phi[\vi u] = 0,
$$ {#eq-general-phase-condition}
where $\Phi$ is a scalar *functional* defined on the space of periodic solutions. The exact choice of phase condition is up to the user, but the most frequent one is the *integral phase condition*
$$
\Phi[\vi u] = \int_0^1 \scal {\vi u(s)} {\dvi v(s)} \dd s,
$$ {#eq-integral-phase-cond}
where $\vi v(s) \in \contf{[0, 1], \R^n}{}$ is the reference period-one solution.

:::{#lem-integral-phase-cond}
The integral phase condition ([-@eq-integral-phase-cond]) is a necessary condition for a local minimum of the time-shift $L_2$-distance of period-1 smooth functions $\vi u, \vi v : \R \to \R^n$
$$
\rho(\sigma) = \int_0^1 \norm{\vi u(s + \sigma) - \vi v(s)}_2^2 \dd s,
$$
such that the minimum is obtained at shift $\sigma = 0$.
:::

:::{.proof}
See @Kuznetsov2016, lemma 11.
:::

We can now collect all the conditions to obtain the periodic BVP
$$
\rcases{
	\dvi u(s) &= T \vi f(\vi u(s)),\; s \in [0,1] \\
	\vi u(0) &= \vi u(1), \\
	\int_0^1 \scal {\vi u(s)} {\dvi v(s)} \dd s &= 0.
}
$$ {#eq-ode-cycle-bvp}
If $(\vi u(\cdot), \vi T_0) \in \contf {[0,1], \R^n} 1 \times \R$ satisfies ([-@eq-ode-cycle-bvp]), then $\vi x(t) = \vi u\brackets{\frac t {T_0}}$ gives the $T_0$-periodic solution of ([-@eq-autonomous-ode-system]) with $\vi x(0) = \vi u(0)$.

Moreover, for the fundamental matrix solution $\vi M(t)$ of ([-@eq-autonomous-ode-system]) we define $\vi \Gamma(s)$, such that for the monodromy matrix of $L_0$ holds $\vi M(T) = \vi \Gamma(1)$ and
$$
\dvi \Gamma(s) - T \jacobi \vi f(\vi u(s)) \vi \Gamma(s) = \vi 0, \; \vi \Gamma(0) = \vi \ident_n.
$$ {#eq-monodromy-system-normalized}
The eigenvalues of $\vi \Gamma(1)$ correspond exactly to the aforementioned multipliers of the cycle $L_0$. We also define the **adjoint monodromy matrix** $\vi \Psi(1)$ as the solution of
$$
\dvi \Psi(s) - T \jacobi \vi f\Tr(\vi u(s)) \vi \Psi(s) = \vi 0, \; \vi \Psi(0) = \vi \ident_n
$$
evaluated at 1. It follows that
$$
\vi \Psi(s) = \brackets{\vi \Gamma(s)\Inv}\Tr.
$$ {#eq-monodromy-psi-phi}
In general, any solution $\vi \xi \in \contf {[0,1], \R^n} {1}$ of an inhomogeneous linear system
$$
\dvi \xi - T \jacobi \vi f(\vi u(s)) \vi \xi = \vi b(s),
$$ 
where $\vi b \in \contf {\R, \R^n} {0}$, can be written as (by ([-@eq-monodromy-psi-phi]))^[In other words, $\vi \Gamma(s)$ is a *fundamental matrix* of ([-@eq-monodromy-system-normalized]). Moreover, this viewpoint explains ([-@eq-monodromy-variation-of-const]) as a corollary of method of variation of constants.]
$$
\vi \xi(s) = \vi \Gamma(s) \brackets{\vi \xi(0) + \int_0^{s} \vi \Gamma\Inv(\sigma) \vi b(\sigma) \dd \sigma} = \vi \Gamma(s) \brackets{\vi \xi(0) + \int_0^s \vi \Psi\Tr(\sigma) \vi b(\sigma) \dd \sigma}.
$$ {#eq-monodromy-variation-of-const}

:::{#def-simple-cycle}
A cycle $L$ is called **simple** if the trivial multiplier $\mu_1 = 1$ has algebraic multiplicity^[Recall the algebraic multiplicity of an eigenvalue $\lmbd$ of a matrix $\vi A$ is its multiplicity as a root of characteristic polynomial. Furthermore, geometric multiplicity of $\lmbd$ is defined as $\dim \ker (\vi A - \lmbd \ident)$. The algebraic multiplicity is always greater than or equal to the geometric multiplicity for any given eigenvalue $\lmbd$ of $\vi A$.] 1.
:::

Let $\vi q_0, \vi p_0 \in \R^n$ be the left and right eigenvectors of the monodromy matrix corresponding to the trivial multiplier $\mu_1$,
$$
\begin{aligned}
(\vi \Gamma(1) - \ident_n) \vi q_0 = (\vi \Psi(1) - \ident_n) \vi p_0 & = \vi 0, \\
(\vi \Gamma(1) - \ident_n)\Tr \vi p_0 = (\vi \Psi(1) - \ident_n)\Tr \vi q_0 & = \vi 0,
\end{aligned}
$$ 
such that $\norm{\vi p_0}_2 = \norm{\vi q_0}_2 = 1$. Also, $\vi q_0 = c_0 \vi f(\vi u(0))$ for $c_0 \in \R \setminus \set{0}$, which follows from ([-@eq-monodromy-system-normalized]) using ([-@eq-autonomous-ode-period-1]),([-@eq-autonomous-ode-1-periodicity-cond]):
\begin{align*}
	\partialOp {s} \brackets{c_0 \vi f(\vi u(0))} - T \vi f(\vi u(1)) c_0 \vi f(\vi u(0)) &= c_0 \jacobi \vi f(\vi u(0)) \dvi u(0) - T \jacobi \vi f(\vi u(0)) c_0 \vi f(\vi u(0)) \\	
	&= c_0 \jacobi \vi f(\vi u(0)) T \vi f(\vi u(0)) - T \jacobi \vi f(\vi u(0)) c_0 \vi f(\vi u(0)) \\
	&= \vi 0.
\end{align*} 

As we have mentioned, the workflow for localization of a periodic orbit is to start from an initial guess $(\vi u, T)$, which we correct by $(\vi w, S) \in \contf {[0,1], \R^n} {1} \times \R$, i.e.,
$$
(\vi u, T) \mapsto (\vi u + \vi w, T + S),
$$
where $(\vi w(\cdot), S)$ is the solution of the linearized inhomogeneous BVP (for reference see ([-@eq-ode-cycle-bvp]))
$$
\rcases{
	\dvi w(s) - T \jacobi \vi f(\vi u(s)) \vi w - S \vi f(\vi u(s)) &= - \dvi u(s) + T \vi f(\vi u(s)), \; s \in [0,1], \\
	\vi w(0) - \vi w(1) &= - \vi u(1) + \vi u(0), \\
	\int_0^1 \scal {\dvi v(\sigma)} {\vi w(\sigma)} \dd \sigma &= - \int_0^1 \scal {\dvi v(\sigma)} {\vi u(\sigma)} \dd \sigma.
}
$$ {#eq-newton-step-bvp}
The left-hand side of ([-@eq-newton-step-bvp]) can be expressed as a matrix operator of form
$$
\underbrace{\bmtr{
	\oper{\jacobi} - T \jacobi \vi f(\vi u) & -\vi f(\vi u) \\
	\evalOp{0} - \evalOp{1} & 0 \\
	\testInt{\dvi v} & 0
}}_{\oper L_{\vi u, T}} \mtr{
	\vi w \\ S
},
$$
where $\oper{\jacobi}$ denotes the differentiation operator, $\evalOp{a}$ is the evaluation operator at $t = a$, i.e., $\evalOp{a} \vi w = \vi w(a)$, and
$$
\testInt{\dvi v} \vi w = \int_0^1 \scal{\dvi v(\sigma)} {\vi w(\sigma)} \dd \sigma.
$$

By taking $\vi v = \vi u$, where $\vi u$ is a solution of ([-@eq-ode-cycle-bvp]), we get $\dvi v = \dvi u = T \vi f(\vi u)$. Moreover, for the purposes of the following theorem, one might choose $\vi v$ sufficiently close to such $\vi u$ (most importantly when $\vi v$ is a reference period-1 function, for example the solution in the previous step of continuation). Lastly, replacement of $T \vi f(\vi u)$ by $\vi f(\vi u)$ does not change the fundamental properties of the operator $\oper{L}_{\vi u, T}$.

:::{#thm-ode-correction-operator-bijection}
If $(\vi u(\cdot), T)$ corresponds to a simple cycle, then the operator
$$
\oper{L}_{\vi u, T} = \bmtr{
	\oper{\jacobi} - T \jacobi \vi f(\vi u) & - \vi f(\vi u) \\
	\evalOp{0} - \evalOp{1} & 0 \\
	\testInt{\vi f(\vi u)} & 0
},
$$
from $\contf {[0,1], \R^n} {1} \times \R$ into $\contf {[0,1], \R^n} {1} \times \R^n \times \R$ is one-to-one and onto.
:::

:::{.proof}
See @Kuznetsov2016, page 36.
:::

By @thm-ode-correction-operator-bijection we know ([-@eq-newton-step-bvp]) can be collectively rewritten as
$$
\oper{L}_{\vi u, T} \mtr{\vi w \\ S} = \vi A_{\vi u, T},
$$ {#eq-newton-correction-infinite-dim}
where $\vi A_{\vi u, T}$ captures the right-hand side of ([-@eq-newton-step-bvp]), and that $\oper{L}_{\vi u, T}$ is regular. Thus, an appropriate correction $(\vi w, S)$ can indeed be found by Newton's method, see @sec-newton-method (assuming we can find a suitable discretization, as ([-@eq-newton-correction-infinite-dim]) is in fact infinite-dimensional).

This approach can be extended to the **continuation** of a **limit cycle branch** of a system
$$
\dvi x = \vi f(\vi x, \alpha), \; \vi x \in \R^n, \; \alpha \in \R,
$$ {#eq-cycle-continuation-problem}
with respect to (w.r.t.) parameter $\alpha \in \R$ as the solution to the following BVP ($\vi u_{\text{old}}$ denotes the period-1 cycle for the previous value of $\alpha$):
$$
\rcases{
	\dvi u(s) - T \vi f(\vi u(s), \alpha) &= \vi 0, \; s \in [0,1], \\
	\vi u(0) - \vi u(1) &= \vi 0, \\
	\int_0^1 \scal{\vi u(\sigma)} {\dvi u_{\text{old}}(\sigma)} \dd \sigma &= 0.
}
$$ {#eq-ode-continuation-bvp}
Furthermore, from @thm-ode-correction-operator-bijection and implicit function theorem, we obtain that a simple cycle has *locally unique* continuation w.r.t. parameter $\alpha$. Again, we can define a corresponding operator to ([-@eq-ode-continuation-bvp]),
$$
\bmtr{
	\oper{\jacobi}_{\vi u} - T \jacobi_{\vi u} \vi f(\vi u, \alpha) & - \vi f(\vi u, \alpha) & - T \pDeriv {\vi f} {\alpha}(\vi u, \alpha) \\
	\evalOp{0} - \evalOp{1} & 0 & 0 \\
	\testInt{\dvi u_{\text{old}}} & 0 & 0
},
$$ {#eq-ode-continuation-operator}
which then has a one-dimensional null-space for a simple cycle $\vi u$.

To solve ([-@eq-ode-cycle-bvp]) (or ([-@eq-ode-continuation-bvp])) numerically, we have to reduce it to a finite-dimensional problem (as opposed to searching in the infinite-dimensional space of periodic functions $\vi u$), that is, we need to choose a *discretization*. Although shooting, multiple shooting and finite differences are sometimes used, most common and most capable is the method of *orthogonal collocation*.

Consider for simplicity the BVP ([-@eq-ode-cycle-bvp]). We shall introduce a partitioning of the interval $[0,1]$ by $N - 1$ mesh points, i.e.,
$$
0 = s_0 < s_1 < \dots < s_N = 1.
$$
The primary goal of the orthogonal collocation is to approximate the true solution $\vi u$ by piecewise-differentiable continuous function, which is defined as a *vector polynomial* $\vi u^{(j)}(s)$ of maximal degree $m$ within each subinterval $[s_j, s_{j+1}]$, $j = 0, 1, \dots, N-1$. Such polynomials can be specified by $m$ *collocation* points 
$$
s_j < \zeta_{j,1} < \zeta_{j,2} < \dots < \zeta_{j,m} < s_{j+1}
$$
belonging to each subinterval, where the approximate solution must satisfy the time-normalized ODE system ([-@eq-autonomous-ode-period-1]) exactly. That is, we require
$$
\evaluateAt{\deriv {\vi u^{(j)}} s} {s = \zeta_{j,i}} = T \vi f(\vi u^{(j)}(\zeta_{j,i}))
$$ {#eq-collocation-spline-requirements}
for $i = 1, \dots, m$, $j = 0,1,\dots, N-1$. To put it another way, we are trying to find $N$ polynomial splines of degree $m$ to approximate the true limit cycle. As the theory of splines tells us, it is advantageous to represent a given vector polynomial $\vi u^{(j)}$ by vectors of (unknown) solutions
$$
\vi u^{j,k} = \vi u^{(j)}(s_{j,k}) \in \R^n, \; k = 0, 1, \dots, m,
$$
where $s_{j,k}$ are *representation points*^[Throughout the discussion of orthogonal collocation, we will use both *collocation* and *representation* points, which do not (in general) coincide.], i.e., nodes of equidistant partitioning of the interval
$$
s_j = s_{j,0} < s_{j,1} < \dots < s_{j,m-1} < s_{j,m} = s_{j+1},
$$
implying $\vi u^{j,m} = \vi u^{j+1,0}$, given by
$$
s_{j,k} = s_j + \frac k m (s_{j+1} - s_{k}), \; j = 0,1,\dots,N-1, k = 0,1,\dots,m.
$$
This yields the following formulation of $\vi u^{(j)}(s)$ as interpolation between values at representation points,
$$
\vi u^{(j)}(s) = \sum_{i = 0}^n \vi u^{j,i} \lagrPoly_{j,i}(s),
$$ {#eq-collocation-spline-lagrange}
where $\lagrPoly_{j,i}(s)$ are the *Lagrange basis polynomials* on $[s_j, s_{j+1}]$,
$$
\lagrPoly_{j,i}(s) = \prod_{k = 0, k \neq i}^m \frac {s - s_{j,k}} {s_{j,i} - s_{j,k}} \implies \lagrPoly_{j,i}(s_{j,k}) = \indicator{i = k} = \lcases{
	1, & i = k, \\
	0, & i \neq k.
}
$$

Using ([-@eq-collocation-spline-lagrange]), we can translate the problem of finding a vector polynomial ([-@eq-collocation-spline-requirements]) into a problem of determining the coefficients $\vi u^{j,i}$. Similarly, we can discretize the periodicity and phase conditions ([-@eq-autonomous-ode-1-periodicity-cond]), ([-@eq-integral-phase-cond]), yielding respectively
$$
\vi u^{0,0} = \vi u^{N-1, m}
$$ {#eq-collocation-discretized-periodicity}
and
$$
\sum_{j = 0}^{N-1} \sum_{i = 0}^{m-1} \omega_{j,i} \scal{\vi u^{j,i}} {\dvi v^{j,i}} + \omega_{N,0} \scal{\vi u^{N,0}} {\dvi v^{N,0}} = 0,
$$ {#eq-collocation-discretized-phase}
where $\dvi v^{j,i}$ are the values of the derivative of the reference period-1 solution at representation points $s_{j,i}$ and $\omega_{j,i}$ are the *Gauss-Legendre quadrature coefficients*^[Gauss-Legendre quadrature is a method of approximating a definite integral $\int_{-1}^1 f(x) \dd x \approx \sum_{i = 1}^n \omega_i f(x_i)$. The optimal value of $x_i$ and $\omega_i$ for a given value of $n$ can be computed analytically. For more information, see @Brin2020]. By combining ([-@eq-collocation-spline-requirements]), ([-@eq-collocation-spline-lagrange]), ([-@eq-collocation-discretized-periodicity]), and ([-@eq-collocation-discretized-phase]) we get a system ([-@eq-ode-discretized-bvp]) of $nmN + n + 1$ scalar equations for the unknown values $\vi u^{j,i} \in \R^n$ at representation points $s_{j,i}$ (there are $(mN + 1)$ values $\vi u^{j,i}$) and for the unknown period $T$,
$$
\rcases{
	\sum_{i = 0}^m \vi u^{j,i} \lagrPoly'_{j,i}(\zeta_{j,k}) - T \vi f\brackets{\sum_{i = 0}^m \vi u^{j,i} \lagrPoly_{j,i}(\zeta_{j,k})} &= \vi 0_n, \\
	j = 0, 1, \dots, N-1, \; k = 1, \dots, m, & \\
	\vi u^{0,0} - \vi u^{N-1, m} &= \vi 0_n, \\
	\sum_{j = 0}^{N-1}\sum_{i = 0}^{m-1} \omega_{j,i} \scal{\vi u^{j,i}}{\dvi v^{j,i}} + \omega_{N,0} \scal{\vi u^{N,0}} {\dvi v^{N,0}} &= 0.
}
$$ {#eq-ode-discretized-bvp}
The resulting finite-dimensional system ([-@eq-ode-discretized-bvp]) is nonlinear, thus we solve it by Newton's method (or some modification exploiting the block structure of Jacobian, which we will discuss later) -- this problem is well-posed by @thm-ode-correction-operator-bijection and the discussion afterward.

So far, we have translated ([-@eq-ode-cycle-bvp]) into a finite dimensional problem, but we have neglected a discussion of a crucial choice of the collocation points $\set{\zeta_{j,i}}$. The distribution of collocation points affects the approximation, so a natural goal is to minimize it. It can be shown that the optimal choice is position them at so-called *Gauss points*, which are the roots of $m$-th degree *Legendre polynomial* corresponding to the subinterval $[s_j, s_{j+1}]$. For more information, we refer the reader to @Brin2020. These roots originally belong to the interval $[-1, 1]$, but can be easily transformed to each $[s_j, s_{j+1}]$. Furthermore, the Legendre polynomials form an orthogonal system on the interval $[-1, 1]$ (or its transformation), which lends the name to the orthogonal collocation method.

Let us note the collocation at Gauss points leads to very highly accurate approximation of a smooth solution of ([-@eq-ode-cycle-bvp]) by ([-@eq-ode-discretized-bvp]) at *mesh* points, namely
$$
\norm{\vi u(s_j) - \vi u^{j, 0}} = O(h^{2m}),
$$
where $h = \max_{0 \leq j \leq N-1} (s_{j+1} - s_j)$.

Let us turn our attention back to the discretized BVP system ([-@eq-ode-discretized-bvp]) and consider its continuation variant (i.e., the discretization of ([-@eq-ode-continuation-bvp])), which can be written as $\vi H(\vi X) = \vi 0$ for appropriate $\vi H$ and $\vi X = \brackets{\set{\vi u^{j,k}}, T, \alpha} \in \R^{mnN + n + 2}$ corresponding to discretized $\vi u$, period $T$ and the continuation parameter $\alpha$. Its Jacobian matrix $\jacobi \vi H$ reads (for $n = 2$, $N = 3$, and $m = 2$)^[The last row of $\jacobi \vi H$ represents the continuation equation, which have not yet discussed (and therefore, we assume, it can, in theory, depend on all variables).]
$$
{\scriptsize \mtr{
	\vi u^{0,0} & & \vi u^{0,1} & & \vi u^{1,0} & & \vi u^{1,1} & & \vi u^{2,0} & & \vi u^{2,1} & & \vi u^{3, 0} & & T & \alpha \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & & & & & & & & & & & \nonzeroEl & \nonzeroEl & & \\
	\nonzeroEl & \nonzeroEl & & & & & & & & & & & \nonzeroEl & \nonzeroEl & & \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl
}},
$$ {#eq-ode-collocation-jacobian}
where $\nonzeroEl$ denotes a generally non-zero element. Thus $\jacobi \vi H$ is sparse, and because it corresponds to the linear operator ([-@eq-ode-continuation-operator]) it also has a one-dimensional null-space satisfying $\vi H(\vi X) = \vi 0$ (at generic points). Let us note that by later also adding a constraint equation stemming from the chosen *continuation* method, see @sec-continuation, we will get a fully determined system. 

The Jacobian matrix $\jacobi \vi H$ is crucial for Newton's method, and its properties deeply affect the computation. Indeed, assuming the residual formulation of Newton's method ([-@eq-newton-residual]), we need to be able to effectively solve the system $\jacobi \vi H \cdot \vi X = \vi R$. Fortunately, in the ODE case, we can perform so-called *condensation of parameters*, see @Govaerts2005 and @Doedel1991, eliminating the dependence on *intermediate variables* (i.e., $\vi u^{j, k}$ for $k = 1, \dots, m-1$) by means of Gaussian elimination performed on rows. This yields ($\zeroEl$'s mark eliminated elements)
$$
{\scriptsize \mtr{
	\vi u^{0,0} & & \vi u^{0,1} & & \vi u^{1,0} & & \vi u^{1,1} & & \vi u^{2,0} & & \vi u^{2,1} & & \vi u^{3, 0} & & T & \alpha \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	+ & + & \zeroEl & \zeroEl & + & + & & & & & & & & & + & + \\
	+ & + & \zeroEl & \zeroEl & \zeroEl & + & & & & & & & & & + & + \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & + & + & \zeroEl & \zeroEl & + & + & & & & & + & + \\
	& & & & + & + & \zeroEl & \zeroEl & \zeroEl & + & & & & & + & + \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & + & + & \zeroEl & \zeroEl & + & + & + & + \\
	& & & & & & & & + & + & \zeroEl & \zeroEl & \zeroEl & + & + & + \\
	+ & + & & & & & & & & & & & + & + & & \\
	+ & + & & & & & & & & & & & + & + & & \\
	+ & + & \zeroEl & \zeroEl & + & + & \zeroEl & \zeroEl & + & + & \zeroEl & \zeroEl & + & + & + & + \\
	+ & + & \zeroEl & \zeroEl & + & + & \zeroEl & \zeroEl & + & + & \zeroEl & \zeroEl & + & + & + & +
}},
$$ {#eq-ode-collocation-jacobian-decoupled}
which has a *decoupled subsystem* for $+$ signs. Hence, we can first solve the subsystem given by $+$'s, and follow up by solving for the intermediate variables afterward, see @Doedel1991. This significantly reduces the dimensionality and thus also memory requirements.

Assume that we have corrected (in, possibly, multiple steps of the Newton-Raphson method) $\vi X = \brackets{\set{\vi u^{j,k}}, T, \alpha}$ converging onto a new point
$$
\vi X_c = \brackets{\set{\vi u_c^{j,k}}, T_c, \alpha_c}
$$
on the branch of the limit cycle. Then $\vi X_c$ approximates a solution to ([-@eq-ode-continuation-bvp]). Moreover, we can further perform Gaussian elimination on rows of $\jacobi \vi H(\vi X_c)$, see ([-@eq-ode-collocation-jacobian-decoupled]) and @Kuznetsov2016, to obtain
$$
{\scriptsize \mtr{
	\vi u^{0,0} & & \vi u^{0,1} & & \vi u^{1,0} & & \vi u^{1,1} & & \vi u^{2,0} & & \vi u^{2,1} & & \vi u^{3, 0} & & T & \alpha \\
	\nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \zeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \zeroEl & \zeroEl & \zeroEl & \nonzeroEl & & & & & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & \nonzeroEl & \nonzeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & & & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & & & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \nonzeroEl & & & & & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	& & & & & & & & \nonzeroEl & \nonzeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	\ast & \ast & & & & & & & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \star & \star & \nonzeroEl & \nonzeroEl \\
	\ast & \ast & & & & & & & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \star & \star & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & & & & & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & & & & & & & & & & & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl \\
	\nonzeroEl & \nonzeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \zeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl & \nonzeroEl
}}.
$$ {#eq-ode-collocation-jacobian-floquet}
Furthermore, we denote by $\vi P_0$ the matrix block marked by $\ast$ sings and by $\vi P_1$ the block of $\star$'s. Let $\vi u(0) = \vi u^{0,0}$ and $\vi u(1) = \vi u^{N,0}$, then the following holds
$$
\jacobi \vi H(\vi X_c) \cdot \brackets{\set{\vi u^{j,k}}, 0, 0}\Tr = \vi 0 \implies \vi P_0 \vi u(0) + \vi P_1 \vi u(1) = \vi 0.
$$
A careful reader might notice that $\vi P_0$ and $\vi P_1$ belong to the linearized equation ([-@eq-collocation-spline-requirements]), i.e., by ([-@eq-monodromy-system-normalized]) the equality
$$
\vi u(1) = - \vi P_1\Inv \vi P_0 \vi u(0)
$$ {#eq-ode-collocation-monodromy-approx}
approximates
$$
\vi u(1) = \vi \Gamma(1) \vi \Gamma(0)\Inv \vi u(0).
$$
Thus, using $\vi \Gamma(0) = \ident_n$ from ([-@eq-monodromy-system-normalized]), $- \vi P_1\Inv \vi P_0 \approx \vi \Gamma(1) = \vi M(T_0)$ for the monodromy matrix $\vi M(T_0)$ corresponding to the cycle $L_0$ of ([-@eq-autonomous-ode-system]). Computing the eigenvalues of $- \vi P_1\Inv \vi P_0$ then estimates the multipliers of the cycle $L_0$, which determine its stability.

### Semidynamical Systems

Now we shall turn our attention to semidynamical systems induced by DDEs. Throughout this section, we will assume the DDE of the form ([-@eq-dde-problem-autonomous]), 
$$
\dvi x(t) = \vi f(\vi x(t), \vi x(t - \tau_1), \dots, \vi x(t - \tau_H)),
$$
together with $0 = \tau_0 < \tau_1 < \dots < \tau_H = \tau$. Although for the purposes of continuation, a dependence of $\vi f$ on some parameter $\alpha \in \R$ is assumed, here we consider $\alpha$ fixed and omit it from the notation. Let us note this section is primarily based on @Roose2007 and @Sieber2014.

#### Equilibrium Localization {#sec-dde-equilibrium-localization}

The techniques of equilibrium localization for DDEs do not differ much from the ODE case. Indeed, assume we are trying to find a stable equilibrium. By time integration, see @sec-method-of-steps and @alg-method-of-steps, we can get to a point $\vi x_t$ in a close neighborhood of the steady state solution. Then, we can apply the Newton-Raphson method, see @sec-newton-method, to the $n$-dimensional system
$$
\vi f(\vi x^i, \dots, \vi x^i) = \vi 0,
$$ {#eq-dde-equilibrium-newton}
where we use $\vi x^0 = \vi x_t(0)$ as the initial approximation. The iterations of the Newton-Raphson method $\vi x^i$ converge to the steady state $\vi x^*$ for $\vi x^0$ sufficiently close to $\vi x^*$. Naturally, this process can be performed with free parameter $\alpha$ to obtain the continuation curve of the equilibrium by considering ([-@eq-dde-equilibrium-newton]) along with a constraint given by the continuation method, see @sec-continuation.

Suppose again we have found $\vi x^*$ with desired accuracy and that our goal is now to determine its stability. It can be shown that stability of $\vi x^*$ follows stability of (the zero solutions of) the linearized equation, see ([-@eq-dde-linearization]),
$$
\dvi x(t) = \sum_{i = 0}^H \jacobi_{\tau_i} \vi f^* \vi x(t - \tau_i),
$$ {#eq-linearized-dde}
where $\jacobi_{\tau_i} \vi f^* = \jacobi_{\tau_i} \vi f(\vi x^*, \dots, \vi x^*)$. Moreover, the linearized equation ([-@eq-linearized-dde]) is *asymptotically* stable if all roots $\lmbd \in \C$ of the corresponding *characteristic equation*
$$
\det\brackets{\lmbd \ident - \jacobi_{\tau_0} \vi f^* - \sum_{i = 1}^H \jacobi_{\tau_i} \vi f^*} = 0
$$ {#eq-linearized-dde-character-eq}
lie in the open left half-plane, i.e., $\ReOf{\lmbd} < 0$, see @Hale1993. Unfortunately, the equation ([-@eq-linearized-dde-character-eq]) has *infinitely many* roots. However, only *finitely many* roots have a real part larger than a give threshold. Thus, to study the stability of $\vi x^*$ it suffices to calculate only *stability-determining* roots $\lmbd \in \C$, which are roots satisfying $\ReOf{\lmbd} \geq r$ for a given threshold $r < 0$ close to zero.

Although analytical conditions for the stability do exist (and are derived by techniques from complex analysis), we shall omit their theoretical derivation to maintain a focused and concise scope. Recently, there have been a development in numerical methods to approximate the stability-determining (i.e., right-most) characteristic roots of ([-@eq-linearized-dde-character-eq]) using either a discretized *solution* operator $\solOp(t)$ of ([-@eq-linearized-dde]), see @sec-dde-stability and @def-dde-solution-oper, or a discretized *infinitesimal* generator $\infGen$ of the semigroup of the solution operator of ([-@eq-linearized-dde]), see @nte-infinitesimal-generator.

The solution operator $\solOp(t)$ has eigenvalues $\mu(t)$, which are related to the roots of the characteristic equation by $\mu(t) = e^{\lmbd t}$, see @Roose2007. Obviously, to determine the stability of $\vi x^*$, we are interested in the dominant eigenvalues. Moreover, it follows that for $t$ large, the eigenvalues $\mu(t)$ are well separated, which we can exploit in the computation. Below, we describe an algorithm to compute the dominant eigenvalues of $\solOp(\diff t)$, where $\diff t$ is the time-step of a linear multistep (LMS) method.

::: {.callout-note #nte-infinitesimal-generator}
##### Infinitesimal generator

In @sec-dde-stability, we have discussed that $\solOp(t)$ is a one-parameter *strongly continuous semigroup*, see @Roose2007. One can thus define the *infinitesimal generator* $\infGen$ of $\set{\solOp(t)}$, see @Curtain1995, by
$$
\infGen \vi \phi = \lim_{t \to 0^+} \frac {\solOp(t)\vi \phi - \vi \phi} t.
$$
In particular, for the linearized equation ([-@eq-linearized-dde]), the infinitesimal generator becomes
$$
\begin{gathered}
	\infGen \vi x(\tht) = \dvi x(\tht),\\
	\vi x \in \domain \infGen \letDef \set{\vi x \in \contStateSpace \divider \dvi x \in \contStateSpace \; \& \; \dvi x(0) = \sum_{i = 0}^H \jacobi_{\tau_i} \vi f^* \vi x(-\tau_i)}.
\end{gathered}
$$
Note that some algorithms for computation of stability of an equilibrium use a discretized infinitesimal generator instead of the solution operator.
:::

When discretized, either of the discussed operators results in some matrix, for which we need to compute its eigenvalues. Therefore, it is our key concern that this matrix is small or its dominant eigenvalues can be computed efficiently by using an iterative scheme, such as subspace iteration.

A straightforward way to approximate the solution operator is to consider the matrix form of the numerical integration of the linearized equation. Such approach has been developed by Engelborghs et al., see @Engelborghs2002a, @Roose2007 and @Sieber2014, and relies on the linear multistep method^[LMS methods are used for numerically integrating ODEs by dependence on previously evaluated points and as such can be paired with method of steps, see @alg-method-of-steps, for DDEs. However, LMS method can also be rewritten for DDEs directly, which gives a much nicer formulation.].

Let us discretize the delay interval $[-\tau, 0]$ (typically extended slightly to both the left and right, as we shall discuss below) by an equidistant mesh of constant step-length $h$ to approximate the solution operator $\solOp(h)$. Then, the solution $\vi x$ can be represented by a finite discrete set of points $\vi x_i \letDef \vi x(t_i)$ with $t_i = ih$. A $k$-step LMS method for DDEs with step-length $h$ may be written as
$$
\sum_{i = 0}^k \alpha_i \vi x_i = h \sum_{i = 0}^k \beta_i \cdot \brackets{\jacobi_{\tau_0} \vi f^* \vi x_i + \sum_{j = 1}^H \jacobi_{\tau_i} \vi f^* \tilde{\vi x}(t_i - \tau_j)},
$$ {#eq-dde-lms}
where $\alpha_i, \beta_i \in \R$ are parameters and $\tilde{\vi x}(t_i - \tau_j)$ are computed by polynomial interpolation with $s_- \in \N$ and $s_+ \in \N$ points to the left and right, respectively, when $(t_i - \tau_j) \notin \set{t_k}$ (i.e., when $(t_i - \tau_j)$ does not "align" with $t_k$ from the discretization). To be precise, DDE-BIFTOOL uses so-called Nordsieck interpolation, see @Sieber2014.

Clearly, the discretization of the operator $\solOp(h)$ itself is given by the linear map from $[\vi x_l, \dots, \vi x_{k-1}]\Tr$ to $[\vi x_{l+1}, \dots, \vi x_k]\Tr$, where $l = - s_- - \ceil{\tau/h}$ and where the mapping is prescribed by ([-@eq-dde-lms]). This procedure results in a $N \times N$ matrix approximation of the operator $\solOp(h)$, where
$$
N = n(-l + k) \approx n\tau/h.
$$ {#eq-dde-lms-dimension}
Unfortunately, due to our choice of approximation and discretization of the solution operator $\solOp(h)$ at the step-length $h$, which is typically rather small, the eigenvalues $\mu(h)$ of $\solOp(h)$ are not well separated. Nonetheless, we can compute them using the QR method with computational cost $O(N^3)$, from which the roots of the characteristic equation can be calculated. Moreover, there have been derived conditions on the step-length $h$, such that all characteristic roots $\lmbd$ with $\ReOf{\lmbd} \geq r$ for some threshold $r < 0$ are approximated sufficiently accurately, see @Engelborghs2002a.

::: {.callout-note #nte-runge-kutta-discretization}
##### Runge-Kutta discretization

Let us note a discretization induced by Runge-Kutta numerical integrator can also be used, albeit it is less frequent in practice. We refer the interested reader to @Breda2006a.
:::

#### Periodic Orbit Localization {#sec-dde-periodic-orbit-localization}

The localization of periodic orbits in DDEs proceeds similarly to the ODE case, see @sec-ode-periodic-orbit-localization. Indeed, main differences lie in the discretization and normalization to unit period. Let us remark this subsection is based on @Verheyden2005, @Sieber2014 and @Roose2007, but also @Engelborghs2001, @Fairgrieve1991 and @Breda2006.

So far, our discretization scheme, prescribed by *representation* points,
$$
0 = s_0 = s_{0,0} < s_{0,1} < \dots < s_{0, m-1} < s_{0,m} = s_{1} = s_{1,0} < \dots < s_N = T,
$$
has been designed for a single period interval $[0,T]$ (or $[0,1]$ after period-1 normalization). As such, we can periodically extent it to cover the interval $[-\tau, T]$. There are now two possible, yet fundamentally different approaches.

The first approach, used in DDE-BIFTOOL, see @Sieber2014 or @Roose2007, normalize the system in time to a unit period for the given cycle (analogously as we did for ODEs). Afterwards, we can wrap-around the delays into interval $[0,1]$, which yields the following BVP^[The notation $t \mod [0,1]$ refers to $t - \max{k \in \Z \divider k \leq t}$.]
$$
\rcases{
	\dvi u (\zeta_{j,i}) - T \vi f\brackets{\vi u(\zeta_{j,i}), \vi u \brackets{\brackets{\zeta_{j,i} - \frac {\tau_1} T} \Mod [0,1]}, \dots, \vi u \brackets{\brackets{\zeta_{j,i} - \frac {\tau_H} T} \Mod [0,1]}} &= \vi 0, \\
	j = 0, 1, \dots, N-1, \; k = 1, \dots, m, \qquad & \\
	\vi u(0) - \vi u(1) &= \vi 0, \\
	\int_0^1 \scal {\vi u(\sigma)} {\dvi u_{\text{old}}(\sigma)} \dd \sigma &= 0,
}
$$ {#eq-dde-periodic-orbit-normalized-bvp}
where $\zeta_{j,i}$ are again the *collocation* points, and $\vi u_0$ and $\vi u_1$ are solution segments on $[-\tau, 0]$ and $[1 - \linefrac{\tau} T, 1]$, respectively. The period $T$ is unknown variable. Unfortunately, the resulting linearized system for the BVP ([-@eq-dde-periodic-orbit-normalized-bvp]) has banded structure, which cannot be easily exploited to aid used linear solver. Moreover, the transformation via the modulo operator complicates the computation of Floquet multipliers, because it distorts the included matrix corresponding to the discretized time-integration operator.

The alternative method, see @Verheyden2005 and @Roose2007, relies on the construction of the orthogonal collocation BVP *without* subsequent wrapping of the delays. This produces a larger BVP, as collocation of the delayed segment is also required, 
$$
\rcasesAt{2}{
	\dvi u (\zeta_{j,i}) - T \vi f\brackets{\vi u(\zeta_{j,i}), \vi u \brackets{\zeta_{j,i} - \frac {\tau_1} T}, \dots, \vi u \brackets{\zeta_{j,i} - \frac {\tau_H} T}} &= \vi H^{\text{dde}} (\vi u, T) &&= \vi 0, \\
	j = 0, 1, \dots, N-1, \; k = 1, \dots, m, \qquad & & & \\
	\vi u_0 - \vi u_1 &= \vi H^{\text{per}}(\vi u) &&= \vi 0, \\
	\int_0^1 \scal {\vi u(\sigma)} {\dvi u_{\text{old}}(\sigma)} \dd \sigma &= \vi \Phi [\vi u] &&= 0
} = \vi H(\vi u, T) = \vi 0.
$$ {#eq-dde-periodic-orbit-unwrapped-bvp}

Note that the trajectory $\vi u(s)$ of the periodic orbit for $s \in [-\tau/T, 1]$ may be split into 2 parts, $\vi u_0$ corresponding to the initial point, which captures the delay, and $\vi u_t(s)$ for $s \in [0, 1]$ representing the cycle itself. To solve BVP ([-@eq-dde-periodic-orbit-unwrapped-bvp]), one can again employ the Newton-Raphson method in its residual form, see @sec-newton-method, where its linearization may be written as
$$
\rcases{
	\vi H^{\text{dde}}_{\vi u_0} \diff \vi u_0 + \vi H^{\text{dde}}_{\vi u_t} \diff \vi u_t + \vi H^{\text{dde}}_{T} \diff T &= - \vi H^{\text{dde}}, \\
	\diff \vi u_0 - \diff \vi u_1 &= - \vi H^{\text{per}}, \\
	\vi \Phi_0 \diff \vi u_0 + \vi \Phi_t \diff \vi u_t + \vi \Phi_T \diff T &= \vi \Phi,
}
$$ {#eq-linearized-dde-periodic-orbit-bvp}
where $\vi H^{\text{dde}}_{\vi u_0}$, $\vi H^{\text{dde}}_{\vi u_t}$ and $\vi H^{\text{dde}}_{T}$ denote the partial derivatives of $\vi H^{\text{dde}}$ with respect to $\vi u_0$, $\vi u_t$ and $T$, respectively, and analogously for $\vi \Phi$. All functions and their partial derivatives are assumed to be evaluated for a given $(\vi u, T)$, such that $\diff \vi u_0, \diff \vi u_t, \diff T$ are considered to be correction variables for ([-@eq-linearized-dde-periodic-orbit-bvp]) in the context of the Newton-Raphson method (this notation was introduced in @sec-newton-method).

::: {.callout-tip #tip-periodic-orbit-dde-to-ode-bvp}
##### Similarities with ODEs

By omitting terms corresponding to partial derivatives with respect to $\vi u_0$, we get a similar expression to the linearized BVP for the ODE case ([-@eq-newton-step-bvp]).
:::

In @Verheyden2005, Verheyden and Lust proposed a method of condensation of parameters for ([-@eq-linearized-dde-periodic-orbit-bvp]), such that the BVP is reduced to the following form
$$
\mtr{
	\vi \Gamma(1) - \ident & \ast \\
	\ast & \ast
} \mtr{\diff \vi u_0 \\ \diff T} = - \vi R,
$$ {#eq-dde-condensed-linearized-cycle-bvp}
where $\vi \Gamma(1)$ is the monodromy matrix, see ([-@eq-monodromy-system-normalized]), which can be obtained from the time-integration matrix $\vi \Gamma_t$ for variational equations^[The variational equations for a given cycle were introduced in @def-variational-equation-about-cycle and correspond to the linearization of the orthogonal collocation BVP.],
$$
\vi \Gamma_t = - \brackets{\vi H^{\text{dde}}_{\vi u_t}}\Inv \vi H^{\text{dde}}_{\vi u_0},
$$ {#eq-dde-time-integration-matrix}
similarly to the ODE case, see ([-@eq-ode-collocation-monodromy-approx]). The exact form of ([-@eq-dde-condensed-linearized-cycle-bvp]) can be found in @Verheyden2005. The Newton-Picard method, see @Lust1998, @Linge2017 or @Hellevik2020, can be used to efficiently solve ([-@eq-dde-condensed-linearized-cycle-bvp]). Afterwards, $\diff \vi u_t$ may be obtained from the first equation of ([-@eq-linearized-dde-periodic-orbit-bvp]). Lastly, Floquet multipliers of the periodic orbit can be computed as the dominant eigenvalues of the discretized monodromy matrix $\vi \Gamma(1)$.

<!-- We can augment ([-@eq-ode-continuation-bvp]) by continuation method equation (Moore-Penrose or pseudo-arc-length) to get a fully-determined system -->


## Continuation {#sec-continuation}

TODO, see @Kuznetsov2023

Let us now explore the *continuation* process itself. 

### Pseudo-arclength Continuation


![Illustration of pseudo-arclength continuation (adapted from @Pribylova2021).](../assets/cont_pa.svg)

TODO

### Moore-Penrose Continuation

TODO

![Illustration of Moore-Penrose continuation (adapted from @Pribylova2021).](../assets/cont_mp.svg)

## Cycle Continuation in Coupled Oscillators

As we have discussed in the @sec-problem-formulation on the problem formulation, we shall again consider the problem of computing phase-shifts between 2 weakly coupled oscillators with possible delay in the coupling. Let us disclose this section is heavily based on the article by Z√°thureck√Ω et al. [-@Zathurecky2025].

For soundness of the grid-based approach to the computation of phase-shift between the oscillators of ([-@eq-general-delayed-coupled-oscillators]), we have simply used the continuous dependence on parameters and initial conditions. Sadly, this argument is not enough for the cycle-continuation-based approach, as nothing a priori guarantees the (local) existence of a suitable cycle near zero coupling strength, which could be used as the starting point for the continuation.

Notably, Z√°thureck√Ω et al. [-@Zathurecky2025] have shown that there must locally exist a cycle manifold (with respect to coupling strength and an arbitrary parameter effecting the frequency of one oscillator) near zero coupling between the two oscillators, such that their phases are synchronized. In particular, we shall focus primarily on the anti-phase synchronization, which may be used to explain the emergence of frequencies higher than physiological, see @Pribylova2024 and @Sevcik2024. This will be performed mainly by studying the phase shifts between the two oscillators.
