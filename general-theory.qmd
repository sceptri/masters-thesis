---
engine: julia
---

:::{.content-visible when-format="pdf"}
\HeaderNumbered
:::


:::{.hidden}
{{< include mathematics.qmd >}}

```{julia}
#| echo: false
#| output: false

using Pkg
Pkg.activate(".")
```
:::

:::{.content-hidden unless-format="html"}
```{julia}
#| echo: false
#| output: false

using WGLMakie
WGLMakie.activate!()
```
:::

:::{.content-hidden when-format="html"}
```{julia}
#| echo: false
#| output: false

using CairoMakie
CairoMakie.activate!()
```
:::

# A Primer on Dynamical Systems {#dynamical-systems-theory}

We have motivated the entire thesis with the usefulness of the knowledge and of the understanding of synchronization in neuroscience. However, we will not describe any experiments performed on real couples of neurons in a lab. Instead, we shall deal with the mathematical abstraction for the studied object, e.g. the coupled neurons.

This abstraction is typically called a (mathematical) model (of the reality). The model should, in theory, capture all the important characteristics of the underlying reality. If the state of the model causally evolves in time, e.g. a model of neuron starts spiking, we often use a *dynamical system* as the model. On the other hand, if the governing rules are noncausal due to the dependence on some past states of the system, the arising dynamical system becomes infinite dimensional. For better "mathematical ergonomics" we will thus define a *semidynamical system*.

## Dynamical Systems {#sec-dynamical-systems}

First and foremost, we will formally introduce the notion of a *dynamical system*, along with its properties. This section is mostly based on @Kuznetsov2023 and @Pribylova2021, with @Sevcik2021 being a valuable inspiration for the structure of this section.

:::{#def-dynamical-system}
##### Dynamical system

A *dynamical system* is a triple $\set{\timeSet, \stateSpace, \evolOp^t}$, where $\timeSet \subseteq \R$ (*time*) endowed with addition $+$ is a subgroup of $(\R, +)$, $\stateSpace$ is a metric space (called a *state space*), and $\set{\evolOp^t}_{t \in \timeSet}$ is a family of evolution operators parametrized by $t \in \timeSet$, such that $\evolOp^t : \stateSpace \to \stateSpace$ maps a certain point $x^0 \in \stateSpace$ to some other state $x_t = \evolOp^t x^0 \in \stateSpace$.
:::

<!-- TODO: Depending on whether time is a group or a monoid, it is either a dynamical or semidynamical system -->

In the @def-dynamical-system, the time set $\timeSet$ can take on various forms. In ecology, we often see a discrete $\timeSet = \Z$ representing a yearly interval between measurements of our system^[Let us note time set $\timeSet = \N_0$ is also common ecology, but such choice leads to semidynamical systems, discussed in more detail in @sec-time-monoid-dynamical-system.]. On the other hand, in physics (and neuroscience) we typically employ $\timeSet = \R$ as we are concerted with even the shortest time intervals and associated changes. Similarly, the exact choice of the state space $\stateSpace$ is dependent of the system in question, but typically we use $\R^n$.

Right now, nothing in the @def-dynamical-system guarantees the system does not abruptly change state, because in general $x \neq \evolOp^0 x$. If this equality does not hold for at least one $x \in \stateSpace$, then such system is called *stochastic*.

:::{#def-deterministic-dynamical-system}
##### Deterministic dynamical system

A dynamical system, see @def-dynamical-system, is called *deterministic* if and only if the following condition is fulfilled
$$
\evolOp^0 = \id,
$$ {#eq-det-dyn-sys}
in other words $\forall x \in \stateSpace: x = \evolOp^0 x$.
:::

Onwards, we will predominantly use deterministic dynamical systems. Another assumption we shall make throughout this thesis is that the "laws of nature" do not change in time, i.e., we presume the dynamical system in question is autonomous (although it may depend on the past).

:::{#def-autonomous-dynamical-system}
##### Autonomous dynamical system

A deterministic dynamical system, see @def-deterministic-dynamical-system, is called *autonomous* if and only if the following condition is fulfilled
$$
\forall t,s \in \timeSet: \evolOp^{t+s} = \evolOp^{t} \circ \evolOp^{s},
$$ {#eq-autonomous-dyn-sys}
in other words $\forall x \in \stateSpace \, \forall t,s \in \timeSet: \evolOp^{t+s} x = \evolOp^t (\evolOp^s x)$.
:::

Most often, a dynamical system is given implicitly by a *differential* or *difference* equation. For example, in population dynamics, one of the earliest models, the *Verhulst model of population*[^verhulst], can prescribed by an ordinary differential equation (ODE)
$$
\dot{x}(t) = \deriv {x(t)} t = x(t) \cdot r_0 \cdot \brackets{1 - \frac {x(t)} K},
$$ {#eq-ode-verhulst}
or a *difference equation*
$$
x(t+1) = x(t) \cdot r_0 \cdot \brackets{1 - \frac {x(t)} K}.
$$ {#eq-difference-verhulst}

[^verhulst]: The equation ([-@eq-ode-verhulst]) describes the [Verhulst](https://en.wikipedia.org/wiki/Pierre_Fran%C3%A7ois_Verhulst) model of a population (and its growth, characterized by $r_0$) in some closed environment with some finite capacity (controlled by $K$).

### Basic Concepts {#sec-basic-concepts}

In this section, we shall introduce basic concepts regarding dynamical systems including, but not limited to, notions of certain special solutions and their stability. Little comment beside the definitions themselves will be provided, as an interested reader can find much more in @Pribylova2021, @Hartman2002 or @Kuznetsov2023.

:::{#def-orbit}
##### Orbit

An *orbit (trajectory)* with an *initial point* $x_0 \in \stateSpace$ is an ordered subset of the state space $\stateSpace$,
$$
\Orbit{x_0} = \set{x \in \stateSpace \divider x = \evolOp^t x_0, \forall t \in \timeSet \text{ such that } \evolOp^t x_0 \text{ is defined}}
$$

In the case of a continuous dynamical system, the orbits are *oriented curves* in the state space. For a discrete dynamical systems, they become sequences of points in $\stateSpace$.
:::

:::{#def-phase-portrait}
##### Phase portrait
A *phase portrait* of a dynamical system is a partitioning of the state space into trajectories.
:::

:::{#def-equilibrium}
##### Equilibrium

A point $x^* \in \stateSpace$ is called an *equilibrium* (fixed point) if $\evolOp^t x^* = x^*$ for all $t \in \timeSet$.
:::

:::{#def-cycle}
##### Cycle

A *cycle* is a periodic orbit, namely a non-equilibrium orbit $L$, such that each point $x_0 \in L$ satisfies $\evolOp^{t + T} x_0 = \evolOp^{t} x_0$ with some $T > 0$, for all $t \in \timeSet$. The smallest admissible $T$ is called the *period* of the cycle $L$.
:::

:::{#def-invariant-set}
##### Invariant set

An *invariant set* of a dynamical system $\set{\timeSet, \stateSpace, \evolOp^t}$ is a subset $\obj S \subset \stateSpace$ which satisfies
$$
x \in \obj{S} \implies \evolOp^t x \in \obj{S} \; \forall t \in \timeSet.
$$
:::

:::{#def-limit-point}
##### $\omega$-limit and $\alpha$-limit point

A point $x_* \in \stateSpace$ is called an *$\omega$-limit point* (resp. $\alpha$-limit point) of the orbit $\Orbit{x_0}$ starting at $x_0 \in \stateSpace$ if their exists a sequence of times $\seqnc{t}{k}{1}{\infty} \subseteq \timeSet$ with $t_k \to \infty$ (resp. $t_k \to - \infty$), such that
$$
\evolOp^{t_k} x_0 \onBottom{\longrightarrow}{k \to \infty} x_*.
$$
:::

:::{#def-limit-set}
##### $\omega$-limit and $\alpha$-limit set

A set $\obj \Omega(\Orbit{x_0})$ of all $\omega$-limit points of the orbit $\Orbit{x_0}$, see @def-limit-point, is called an $\omega$-limit set. Similarly, a set $\obj A(\Orbit{x_0})$ of all $\alpha$-limit points of the orbit $\Orbit{x_0}$ is called an $\alpha$-limit set.

Lastly, a set $\limitSet(\Orbit{x_0}) = \obj \Omega(\Orbit{x_0}) \cup \obj A(\Orbit{x_0})$ of all limit points of the orbit $\Orbit{x_0}$ is called its *limit set*.
:::

<!--  FIXME: Add, but I am not sure of the translation
:::{#def-limit-invariant-loop}
##### Limit invariant loop

::: -->

:::{#def-limit-cycle}
##### Limit cycle

A cycle of a continuous-time dynamical system, in a neighborhood of which there are no other cycles, is called a *limit cycle*.
:::

::: {.callout-note #nte-limit-cycle}
##### Equivalent definition of a limit cycle

Equivalently to @def-limit-cycle, one can define a *limit cycle* as a cycle, which is the limit set, see @def-limit-set, of orbits in its neighborhood.
:::

<!-- TODO: Add citations (use files/, Kuznetsov...) -->

:::{#def-invariant-set-stable}

An invariant set $S_0$ is called 

1. **Lyapunov stable** if for any sufficiently small neighborhood $U \supset S_0$ there exists a neighborhood $V \supset S_0$ such that $\evolOp^t x \in U$ for all $x \in V$ and all $t > 0$;
2. **asymptotically stable** there exists a neighborhood $U_0 \supset S_0$ such that $\evolOp^t x \to S_0$ for all $x \in U_0$, as $t \to +\infty$;
3. **stable** if it is both *Lyapunov* and *asymptotically stable*;
4. **unstable** if it is not *stable*.
:::

Stable invariant set is called an *attractor*, whereas if the invariant set is unstable it is called a *repeller*.

#### Topologically Equivalent Dynamical Systems

<!-- TODO: Add refs to section -->
So far, we have described dynamical systems mainly in general terms. Later, we will get to concrete examples of dynamical systems, primarily from the neuroscience field, which will be too complex to apply certain techniques from bifurcation theory directly. Hence, we introduce the notion of (local) topological equivalence to remedy this issue.

:::{#def-topological-equivalence}
##### Topological equivalence

A dynamical system $\obj D_1 = \set{\timeSet, \R^n, \evolOp^t}$ is said to be *topologically equivalent* to a dynamical system $\obj D_2 = \set{\timeSet, \R^n, \oper{\psi}^t}$, if there exists a *homeomorphism*[^homeomorphism] $\vi h: \R^n \to \R^n$, which maps orbits of system $\obj D_1$ to orbits of system $\obj D_2$, such that their orientation is kept. In such case, their phase portraits are called *(topologically) equivalent*.
:::

[^homeomorphism]: In the context of topology, a *homeomorphism* (also called a *bicontinuous function*) is a bijective and continuous function, such that its inverse is also continuous.

:::{#def-local-topological-equivalence}
##### Local topological equivalence

A dynamical system $\obj D_1 = \set{\timeSet, \R^n, \evolOp^t}$ is said to be *locally topologically equivalent* in the neighborhood of its equilibrium $\vi x^*$ to a dynamical system $\obj D_2 = \set{\timeSet, \R^n, \oper{\psi}^t}$ in the neighborhood of its equilibrium $\vi y^*$, if there exists a *homeomorphism* $\vi h: \R^n \to \R^n$, such that

1. $\vi h$ is defined on a (small) neighborhood $\neigh{\vi x^*} \subset \R^n$,
2. satisfies $\vi y^* = \vi h(\vi x^*)$ and
3. maps orbits of dynamical system $\obj D_1$ in the neighborhood $\neigh{\vi x_0}$ to orbits of system $\obj D_2$ in the neighborhood $\neigh{\vi y^*}$, such that their orientation is kept.
:::

### Continuous-time Autonomous Systems

:::{#def-autonomous-ode-system}
An *autonomous system of (ordinary) differential equations* is a system of form
$$
\dvi x = \vi f(\vi x),
$$ {#eq-autonomous-ode-system}
where $\vi x \in \stateSpace = \R^n$ and a vector-valued function $\vi f: \R^n \to \R^n$ is sufficiently smooth. The symbol $\dvi x$ denotes a derivative of $\vi x(t)$ with respect to time $t \in \timeSet = \R$.

The system of ODEs ([-@eq-autonomous-ode-system]) induces a *continuous-time autonomous dynamical system*.
:::

::: {.callout-tip #tip-ode-vector-field}
##### Vector field

The function $\vi f$ is called a *vector field*, as it maps a vector $\vi f(\vi x)$ to each point $\vi x$ of the state space. See @fig-vector-field for an example.
:::

```{julia}
#| label: fig-vector-field
#| fig-cap: Vector field (or a phase portrait) corresponding to the dynamical system $\dot{x} = -x, \dot{y} = 2y$.
#| width: 60%
fig = Figure(size=(450, 350))
ax = Axis(fig[1,1])
streamplot!(ax, (x,y) -> Point2f(-x, 2y), -2..4, -2..2, colormap=:Zissou1)

fig
```

:::{#thm-lyapunov-ode}
##### Lyapunov

Consider a dynamical system ([-@eq-autonomous-ode-system]) with an equilibrium $\vi x^*$. Let
$$
\Jacobi^* = \jacobi \vi f(\vi x^*) = \mtr{
	\pDeriv {f_1} {x_1} (\vi x^*) & \pDeriv {f_1} {x_2} (\vi x^*) & \dots & \pDeriv {f_1} {x_n} (\vi x^*) \\
	\pDeriv {f_2} {x_1} (\vi x^*) & \pDeriv {f_2} {x_2} (\vi x^*) & \dots & \pDeriv {f_2} {x_n} (\vi x^*) \\
	\vdots & \vdots & \ddots & \vdots \\
	\pDeriv {f_n} {x_1} (\vi x^*) & \pDeriv {f_n} {x_2} (\vi x^*) & \dots & \pDeriv {f_n} {x_n} (\vi x^*)
}
$$
denote a Jacobian matrix evaluated at $\vi x^*$. Then $\vi x^*$ is *stable*, if all eigenvalues $\lmbd_i$, where $i \in \oneToN{n}$^[For conciseness, we use the following notation $\oneToN{n} := \set{1, \dots, n}.$], of the matrix $\Jacobi^*$ satisfy $\reOf{\lmbd_i} < 0$.
:::

:::{.proof}
See @Chicone2006, page 160, or @Perko2001, page 185.
:::

:::{#def-hyperbolic-equilibrium-cont}
##### Hyperbolic equilibrium

An equilibrium $\vi x^*$ of the system ([-@eq-autonomous-ode-system]) is called *hyperbolic*, if none of the eigenvalues corresponding to the Jacobian matrix $\Jacobi^* = \jacobi \vi f(\vi x^*)$ lies on the imaginary axis.
:::

:::{#thm-grobman-hartman-cont}
##### Grobman-Hartman

The system ([-@eq-autonomous-ode-system]) in a neighborhood of its *hyperbolic* equilibrium $\vi x^*$ is *locally topologically equivalent*, in the sense of @def-local-topological-equivalence, to its linearization
$$
\dvi x = \jacobi \vi f(\vi x^*) \vi x.
$$ {#eq-linearization}
:::

:::{.proof}
See @Chicone2006, page 306, or @Perko2001, page 120.
:::

#### Lyapunov's Direct Method

:::{#def-lpd-function}
A function $V: \R^n \to \R$ is called *locally positive-definite (LPD)* at $\vi x^*$, if the following holds:

1. $V(\vi x^*) = 0$,
2. $V(\vi x) > 0$, $\vi x \in \neigh{\vi x^*} \setminus \set{\vi x^*}$ for some neighborhood $\neigh{\vi x^*}$.

If only $V(\vi x) \geq 0$ holds on a neighborhood $\neigh{\vi x^*}$, then the function is called *locally positive semi-definite (LPSD)*. Analogously, we can define a *locally negative (semi-)definite* function.
:::

:::{#def-lyapunov-function}
###### Lyapunov function

Let $\vi \vf(t; \vi x_0)$ be a solution of the system ([-@eq-autonomous-ode-system]) together with the initial $\vi x(0) = \vi x_0$. A function $V: \R^n \to \R$ is called *Lyapunov* at $\vi x^*$, if $V$ is locally positive definite and also $\forall \vi x_0 \in \neigh{\vi x^*}$ is the function $V \circ \vi \vf(t; \vi x_0)$ *non-increasing* for all $t > 0$.

Moreover, the function $V$ is called *strictly Lyapunov* if $V \circ \vi \vf(t; \vi x_0)$ is *(strictly) decreasing* for all $t > 0$.
:::

::: {.callout-note #nte-lyapunov-function}
##### Monotonicity of $V \circ \vi \vf(t; \vi x_0)$

Equivalently to requiring a non-increasing $V \circ \vi \vf(t; \vi x_0)$, one can instead demand the *derivatives with respect to the trajectories* of $V$, i.e. $\dot V(\vi x(t))$, to be non-positive. In other words, the *derivative w.r.t. the trajectories* $\dot V$ must be *locally negative semi-definite*.
:::

:::{#thm-lyapunov-stability-theorem}
##### Lyapunov's direct method

If $\vi x^*$ is an equilibrium of the system ([-@eq-autonomous-ode-system]) and $V$ is a *Lyapunov* function for the system at $\vi x^*$, then $\vi x^*$ is *Lyapunov stable*. If, in addition, $V$ is a *strict Lyapunov* function, then $\vi x^*$ is *stable*.
:::

:::{.proof}
See @Chicone2006, page 24.
:::

### Discrete Dynamical Systems

:::{#def-autonomous-difference-system}

An *autonomous system of difference equations* is a system of form
$$
\vi x \mapsto \vi f(\vi x) \quad \iff \quad \vi x_{m+1} = \vi f(\vi x_m),
$$ {#eq-autonomous-difference-sys}
where $\vi x, \vi x_m \in \stateSpace = \R^n$ and the function $\vi f : \R^n \to \R^n$ is sufficiently smooth.

The system ([-@eq-autonomous-difference-sys]) induces a *discrete-time autonomous dynamical system*.
:::

:::{#thm-lyapunov-differences}
##### Lyapunov, analogous to @thm-lyapunov-ode

Consider a dynamical system ([-@eq-autonomous-difference-sys]) with a fixed point $\vi x^*$. Let $\Jacobi^* = \jacobi \vi f(\vi x^*)$ denote the Jacobian matrix evaluated at $\vi x^*$. Then $\vi x^*$ is stable, if all eigenvalues^[Eigenvalues of fixed points of discrete-time dynamical systems are often called *multipliers*, see @Sevcik2021.] $\lambda_i$, where $i \in \oneToN{n}$, of the matrix $\Jacobi^*$ satisfy $\absval{\lambda_i} < 1$.
:::

:::{.proof}
See @Elaydi2005, page 222.
:::

:::{#def-hyperbolic-equilibrium-discrete}
##### Hyperbolic fixed point

A fixed point $\vi x^*$ of the system ([-@eq-autonomous-difference-sys]) is called **hyperbolic**, if none of the eigenvalues corresponding to the Jacobian matrix $\Jacobi^* = \jacobi \vi f(\vi x^*)$ has unit magnitude.
:::

:::{#thm-grobman-hartman-discrete}
##### Grobman-Hartman, analogous to @thm-grobman-hartman-cont

The system ([-@eq-autonomous-difference-sys]) is locally topologically equivalent in the neighborhood of its *hyperbolic* fixed point $\vi x^*$ to its linearization
$$
\vi x \mapsto \jacobi \vi f(\vi x^*) \vi x.
$$
:::

:::{.proof}
See @Chicone2006, page 311.
:::

:::{#exm-fixed-points-2d}
##### Fixed points of two-dimensional discrete-time dynamical system

As an example, consider a two-dimensional discrete-time dynamical system
$$
\vi x_{m+1} = \vi f(\vi x_{m}),
$$ {#eq-2d-discrete-dyn-sys}
where $\vi x_m = (x_{m, 1}, x_{m, 2})\Tr$ and $\vi f : \R^2 \to \R^2$ is smooth. Moreover, let us assume there exists a *hyperbolic equilibrium* $\vi x^* = \vi f(\vi x^*)$ of the system ([-@eq-2d-discrete-dyn-sys]) and let $\Jacobi^* = \jacobi \vi f(\vi x^*)$ denote the corresponding Jacobian matrix evaluated at $\vi x^*$. Then $\Jacobi^*$ has two eigenvalues $\lambda_1, \lambda_2$, which satisfy
$$
\det \brackets{\Jacobi^* - \lambda \ident_2} = \lambda^2 - \trace \Jacobi^* \lambda + \det \Jacobi^*.
$$
Here, $I_2$ corresponds to a 2-by-2 identity matrix, $\trace \Jacobi^* = \lambda_1 + \lambda_2$  is the trace of the  determinant and finally, $\det \Jacobi^* = \lambda_1 \lambda_2$ denotes the determinant of $\Jacobi^*$.

On a different note, in the case of a linear continuous-time dynamical system, i.e.
$$
\dvi y = \vi A \vi y,
$$
where $\vi y \in \R^n$ and $\vi A \in \R^{n\times n}$, with an equilibrium $\vi y^*$ we can partition the state-space into disjoint linear (vector) subspaces:

- *stable subspace* $\obj{E}^s = \Span{\vi v^1, \dots, \vi v^{n_{-}}}$ of dimension $n_{-}$, where $\vi v^1, \dots, \vi v^{n_{-}}$ are the eigenvectors of $\vi A$ corresponding to eigenvalues with *negative* real parts;
- *unstable subspace* $\obj{E}^u = \Span{\vi u^1, \dots, \vi u^{n_{+}}}$ of dimension $n_{+}$, where $\vi u^1, \dots, \vi u^{n_{+}}$ are the eigenvectors of $\vi A$ corresponding to eigenvalues with *positive* real parts;
- *central subspace* $\obj{E}^c = \Span{\vi w^1, \dots, \vi w^{n_0}}$ of dimension $n_{0}$, where $\vi w^1, \dots, \vi w^{n_0}$ are the eigenvectors of $\vi A$ corresponding to *strictly imaginary* eigenvalues, i.e. eigenvalues with zero real part.

Then it holds that $n_{-} + n_{+} + n_0 = n$. Similarly, for linear discrete-time dynamical systems, one can perform an equivalent partitioning, such that $\obj{E}^s$ is spanned by eigenvectors corresponding to eigenvalues *lying inside the unit circle* (on the imaginary plane). Furthermore, $\obj{E}^u$ and $\obj{E}^c$ can be defined for discrete-time dynamical systems analogously.

By @thm-grobman-hartman-discrete, we know that the system ([-@eq-2d-discrete-dyn-sys]) is locally topologically equivalent to its linearization in a neighborhood of its hyperbolic fixed point $\vi x^*$. Interestingly, we can classify the said fixed point based on the number of stable (and unstable) eigenvectors of the corresponding $\Jacobi^*$, i.e. the classification is based on the dimensions of $\obj E^s$, $\obj E^u$ and $\obj E^c$ of the partitioning of the state-space per linearization.

A fixed point is classified as a *sink*, when both eigenvalues are real and stable. Similarly, it is called a *spiral sink* if both eigenvalues are stable and complex. Should one eigenvalue be stable and the other unstable, the resulting fixed point is a *saddle*. Finally, an equilibrium is a *source*, resp. *spiral source*, when both eigenvalues are unstable and *real*, resp. *complex*. For an overview, see @fig-2d-discrete-dynamical-system.

```{julia}
#| fig-cap: "Orbits near different equilibria of a linear discrete-time dynamical system (time flows from blue to red)."
#| label: fig-2d-discrete-dynamical-system
#| width: 95%

function integrate(A, x, n)
	X = similar(x, n, 2)
	X[1, :] = x
	for i = 2:n
		X[i, :] = A*X[i - 1, :]
	end
	return X
end

fig = Figure(size = (1000, 200), backgroundcolor=:transparent)

x0 = [1.0, 1.0]
x_eq = [0.01, 0.01]
n = 12

ax = Axis(fig[1,1], title="sink")
hidedecorations!(ax)
scatterlines!(ax, integrate([0.5 0; 0 0.25], x0, n), colormap=:Zissou1, color=1:n)

ax = Axis(fig[1,2], title="spiral sink")
hidedecorations!(ax)
scatterlines!(ax, integrate([0.5 0.25; -0.25 0.5], x0, n), colormap=:Zissou1, color=1:n)

ax = Axis(fig[1,3], title="saddle")
hidedecorations!(ax)
scatterlines!(ax, integrate([1.25 0; 0 0.25], x0, n), colormap=:Zissou1, color=1:n)
scatterlines!(ax, integrate([1.25 0; 0 0.25], x0 .* [-1.0, 1.0], n), colormap=:Zissou1, color=1:n)

ax = Axis(fig[1,4], title="source")
hidedecorations!(ax)
scatterlines!(ax, integrate([1.5 0; 0 1.25], x_eq, n), colormap=:Zissou1, color=1:n)

ax = Axis(fig[1,5], title="spiral source")
hidedecorations!(ax)
scatterlines!(ax, integrate([1.5 0.75; -0.75 1.5], x_eq, n), colormap=:Zissou1, color=1:n)

fig
```
<!-- TODO: Check color direction in caption correct -->
:::

For completeness sake, we give a classical result from theory of ODEs concerning the existence, uniqueness and smooth dependence on the initial conditions of the solution for a given ODE.

:::{#thm-exists-unique-smooth-ode-solution}
##### Existence, uniqueness and smooth dependence

Consider a system of ordinary differential equations
$$
\dvi x = \vi f(\vi x), \vi x \in \R,
$$
where $\vi f: \R^n \to \R^n$ is smooth in an open region $U \subset \R^n$. Then there is a unique function $\vi x = \vi x(t, \vi x_0)$, $\vi x : \R^1 \times \R^n \to \R^n$, that is smooth in $(t,x)$, and satisfies, for each $\vi x_0 \in U$, the following conditions:

1. $\vi x(0, \vi x_0) = \vi x_0$;
2. there is an interval $\timeInt = (-\delta_1, \delta_2)$, where $\delta_{1,2} = \delta_{1,2}(\vi x_0) > 0$, such that, for all $t \in \timeInt$,
	$$
	\vi y(t) = \vi x(t, \vi x_0) \in U,
	$$
	and
	$$
	\dvi y(t) = \vi f(\vi y(t)).
	$$
:::

:::{.proof}
See @Hartman2002, page 94.
:::


### Poincaré Map

The study of continuous-time dynamical systems naturally leads to discrete-time dynamical systems, be it via sampling the continuous orbit at discrete times, e.g. separated by $\Delta t$ which induces a *time-shift map*. Another way to obtain a discrete-time dynamical system from a continuous-time one is through a so-called *Poincaré map*.

Consider a continuous-time dynamical system of form
$$
\dvi x = \vi f(\vi x), \quad \vi x \in \R^n,
$$ {#eq-poincare-map-system}
where $\vi f$ is smooth and assume that ([-@eq-poincare-map-system]) has a periodic orbit $L_0$. Let $\vi x_0$ be a point on $L_0$ and denote $\crossSection$ the *cross-section* to the cycle at this point, see @fig-poincare-map.

![Poincaré map corresponding to the system ([-@eq-poincare-map-system]), the cycle $L_0$ and its point $\vi x_0$.](diagrams/poincare_maps.drawio.svg){#fig-poincare-map .final}

The cross-section $\crossSection$ is a smooth hypersurface of dimension $n-1$ (thus we say $\codim \crossSection = 1$, i.e. the cross-section hypersurface $\crossSection$ is of "codimension" one), which intersects $L_0$ at a nonzero angle. The nonzero angle requirement is called the *transversality* condition, which effectively dictates that the hypersurface is not parallel to the through-going trajectories, thus the trajectories truly intersect the cross-section $\crossSection$.

Consider now orbits of ([-@eq-poincare-map-system]) near the cycle $L_0$ and recall that the cycle $L_0$ itself is an orbit which starts at point $\vi x_0$ on $\crossSection$ and returns to the same point on $\crossSection$. As the @thm-exists-unique-smooth-ode-solution guarantees the solution of ([-@eq-poincare-map-system]) depend smoothly on its initial condition, an orbit starting at $\vi x \in \crossSection$ sufficiently close to $\vi x_0$ will transversally intersect the hypersurface $\crossSection$ at some other point $\tilde{\vi x}$ near $\vi x_0$. Therefore, this induces a map $\vi P: \crossSection \to \crossSection$,
$$
\vi x \mapsto \tilde{\vi x} = \vi P(\vi x).
$$

:::{#def-poincare-map}
##### Poincaré map

The map $\vi P$ defined above is called a *Poincaré map* associated with the cycle $L_0$.
:::

Similarly, we can characterize the Poincaré map $\vi P$ using a local coordinates $\vi \xi = (\xi_1, \dots, \xi_{n - 1})$ on $\crossSection$, such that the choice $\vi \xi = \vi 0$ corresponds to $\vi x_0$. Then the Poincaré map can be locally defined as a function $\vi P: \R^{n - 1} \to \R^{n - 1}$, which maps $\vi \xi$ corresponding to $\vi x$ to $\tilde{\vi \xi}$ corresponding to $\tilde {\vi x}$, i.e.
$$
\vi P(\vi \xi) = \tilde{\vi \xi}.
$$

In other words, the Poincaré map $\vi P$ prescribes a *discrete-time dynamical system* on the hypersurface $\crossSection$. Its origin $\vi \xi = \vi 0$ is a fixed point of this mapping. To our advantage, the stability of the underlying cycle $L_0$ is then equivalent to the stability of the fixed point $\vi \xi_0 = \vi 0$. By @thm-lyapunov-differences, we know the cycle is thus stable if all eigenvalues (also called *multipliers*) $\mu_1, \dots, \mu_{n - 1}$ of the $(n - 1) \times (n - 1)$ Jacobian matrix of $\vi P$,
$$
\Jacobi_{\vi P} = \jacobi_{\vi \xi} \vi P(\vi \xi_0),
$$
are located inside the unit circle $\norm{\vi \mu} = 1$. The Poincaré map will throughout this thesis paint itself a powerful tool in the bifurcation analysis of dynamical systems, and the following lemma hints at its usefulness.

:::{#lem-poincaré-map}
The multipliers $\mu_1, \dots, \mu_{n - 1}$ of the Jacobian matrix $\Jacobi_{\vi P}$ of the Poincaré map $\vi P$ associated with a cycle $L_0$ are independent of the point $\vi x_0$ on $L_0$, the cross-section $\crossSection$, and local coordinates on it.
:::

:::{.proof}
See @Kuznetsov2023, page 27.
:::

Consider now the cycle $L_0$ and let $\vi x^0(t)$ denote its corresponding periodic solution of ([-@eq-poincare-map-system]) with the period $T_0$, i.e. $\vi x^0(t + T_0) = \vi x^0(t)$. Then any solution $\vi x(t)$ of ([-@eq-poincare-map-system]) can be written as
$$
\vi x(t) = \vi x^0(t) + \vi u(t),
$$
where $\vi u(t)$ stands for the deviation of the solution from the referential periodic solution. Then,
\begin{align*}
\dvi u(t) &= \dvi x(t) - \dvi x^0(t) \\
	&= \vi f(\vi x^0(t) + \vi u(t))  - \vi f(\vi x^0(t)) \\
	&= \Jacobi(t) \vi u(t) + O(\norm{\vi u(t)}^2).
\end{align*}
Omitting $O(\norm{\vi u(t)}^2)$ terms gives us a linear $T_0$-periodic system
$$
\dvi u = \Jacobi(t) \vi u, \quad \vi u \in \R^n,
$$ {#eq-variational-about-cycle}
where $\Jacobi(t) = \jacobi_{\vi x} \vi f(\vi x^0(t))$, and $\Jacobi(t + T_0) = \Jacobi(t)$.

:::{#def-variational-equation-about-cycle}
System ([-@eq-variational-about-cycle]) is called the *variational equation* about the cycle $L_0$.
:::

As the variational equation describes the evolution of perturbations in the proximity of the cycle $L_0$, naturally its stability depends on the properties of the variational equation.

:::{#def-monodromy-matrix}
The time-dependent matrix $\vi M(t)$ is called the *fundamental matrix solution* of ([-@eq-poincare-map-system]) if it satisfies
$$
\dvi M = \Jacobi(t) \vi M,
$$
with the initial condition $\vi M(0) = \ident_n$. The matrix $\vi M(T_0)$ is called a **monodromy matrix** of the cycle $L_0$.
:::

:::{#thm-floquet-exponents}
##### Floquet exponents

The monodromy matrix $\vi M(T_0)$ has eigenvalues (called *Floquet exponents* or *multipliers*)
$$
1, \mu_1, \dots, \mu_{n - 1},
$$
where $\mu_i$ are the multiplier of the Poincaré map $\vi P$ associated with the cycle $L_0$, see @lem-poincaré-map.
:::

:::{.proof}
See @Kuznetsov2023, page 30, for a sketch of the proof.
:::

## Semidynamical Systems

Before delving straight into the semidynamical systems, we have to generalize our notion of a differential equation. This modification will rely on introducing a deviating argument, i.e. some of the derivatives of the differential equation will be evaluated at different argument values. This section will be primarily sourced from @Hale1977, @Hale1993, @Kolmanovskii1992, @Diekmann1995, @Smith2010, and @Guo2013.

### Functional Differential Equations

So far, we have mainly discussed *ordinary differential equations*, which are equations relating the values of an unknown function and some of its derivatives at a single and the same argument value, e.g. $F(t, x, \dot{x}, \ddot{x}) = 0$.

In contrast, a *functional equation* (FE) is an equation connecting outputs of an unknown function evaluated at different argument values. As an example, $x(-t) + 3x(2t) = 0$, $x(x(t)) = x(t) + 2$, and $x(t) = tx(t+1) - \brackets{x(t-2)}^3$ are all examples of functional equations. The differences between the argument values of an unknown function and $t$ (its "default" argument) in a FE are called *argument deviations*. If all argument deviations are constant (like in the last example shown above), then the FE is called a *difference-functional equation*.

Although, we have so far shown only examples of FEs with *discrete* (or *concentrated*) argument deviations, another possibility are FEs with *continuous* and *mixed* (both continuous and discrete) *argument deviations*. They are called *integral* and *integral-functional* (or *integral-difference*) equations.

We can further combine these notions of ordinary differential equations and functional equations, yielding in the process *functional differential equations* (FDEs). FDEs can also contain discrete, continuous or mixed argument deviations. Thus, one can introduce *differential-difference equations*, and *integro-differential equations* in a similar way as seen above.

The aforementioned generalization in its full form leads to *functional differential equations* -- equations describing the relation between the unknown function and some of its derivatives for, in general, different argument values. Our main subject of study will be a subclass of *differential-difference equations* called the *functional differential equations with aftereffects*, which are of form
$$
\vi x^{(m)}(t) = \vi f\brackets{t, \vi x^{(m_1)}(t - \tau_1(t)), \dots, \vi x^{(m_k)}(t - \tau_k(t))},
$$ {#eq-differe}
where $\vi x(t) \in \R^n$, $m_1, \dots, m_k \geq 0$, and $\tau_1(t), \dots, \tau_k(t) \geq 0$. They can be further classified as:

- *retarded FDEs* (RFDEs) or equivalently *delay differential equations* (DDEs), if $\max \set{m_1, \dots, m_k} < m$;
- *neutral FDEs* (NFDEs), if $\max \set{m_1, \dots, m_k} = m$;
- *advanced FDEs* (AFDEs), if $\max \set{m_1, \dots, m_k} > m$.

Let us also note that the argument deviations can, in theory, be dependent on the state, e.g. $\dot{x}(t) = f(t, x(t), x(t - h(t, x(t))))$, but we shall omit them from our considerations.

<!-- TODO: Check that it can really be like this! -->
::: {.callout-caution #cau-dde-convention}
### Note on naming convention

For clarity and brevity, throughout this thesis *RFDEs with (discrete) constant delays* will be called *delay differential equations* (DDEs), although the term can be more general in other literature.
:::

### Method of Steps {#sec-method-of-steps}

All throughout this work, we shall see many parallels between ODEs and DDEs. While their connection is intricate and would require a more thorough discussion, one way it can be partly understood is via the *method of steps* -- a way of solving delay differential equations. This subsection is mainly based on @Smith2010.

To begin gently, we will start with a simple case of $x \in \R$. Let us thus consider a nonlinear DDE of form
$$
\dot x(t) = f(t, x(t), x(t - r))
$$ {#eq-1d-dde}
with a single delay $r > 0$. Assume that $f(t,x,y)$ and $f_x(t,x,y)$ are continuous on $\R^3$. Let $s \in \R$ and a continuous *history* function $\phi: [s - r, s] \to \R$ be given. We aim to find a solution of ([-@eq-1d-dde]) such that
$$
x(t) = \phi(t), \; s-r \leq t \leq s
$$ {#eq-1d-dde-initial-cond}
and satisfying ([-@eq-1d-dde]) on $s \leq t \leq s + \sigma$ for some $\sigma > 0$. As a careful reader might notice, we must interpret $\dot x(s)$ as a *right-hand* derivative at $s$.

The system of equations ([-@eq-1d-dde]) and ([-@eq-1d-dde-initial-cond]) can now be solved with the so-called *method of steps* as follows. For $s \leq t \leq s + r$, $x(t)$ must satisfy the initial-value problem for the following ODE:
$$
\dot y(t) = \underbrace{f(t, y(t), \phi(t - r))}_{g(t,y)}, \; y(s) = \phi(s), \; s \leq t \leq s + r.
$$

From the continuity of $f$ and $f_x$ immediately follows the continuity of $g$ and $g_y$, thus existence of an unique local solution is guaranteed by classic results from the ODE theory, see, for example, @Chicone2006. Moreover, if this local solution $x(t)$ exists on the entire interval $s \leq t \leq s + r$, then, together with the history $\phi$, we know the solution $x(t)$ of ([-@eq-1d-dde])-([-@eq-1d-dde-initial-cond]) on $[s-r, s+r]$ and we may repeat the presented argument to extend our solution to the right. Indeed, considering now the interval $s+r \leq s \leq s + 2r$, a solution of $x(t)$ of ([-@eq-1d-dde])-([-@eq-1d-dde-initial-cond]) must now satisfy the following ODE problem:
$$
\dot y(t) = f(t, y(t), x(t - r)), \; y(s+r) = x(s+r), \; s+r \leq t \leq s+2r.
$$
Just as before, from continuity of $f$ and $f_x$ yields a local unique solution, by abuse of notation called again $x(t)$, defined on some subinterval $[s+r, \sigma) \subset [s+r, s + 2r]$ or, possibly, the entire interval. Naturally, $x(t)$, now existing on $[s-r, \sigma)$ where $\sigma > s + r$, is again a solution of ([-@eq-1d-dde])-([-@eq-1d-dde-initial-cond]). Finally, if the solution exists on the entire interval $[s+r, s+2r]$ we can perform another *step*, i.e. repeat this procedure, to extend $x(t)$ further to the right to $[s+2r, s+3r]$ or some subinterval. 

:::{#thm-1d-single-delay-method-of-steps}
##### Single-delay method of steps on $\R$

Let $f(t,x,y)$ and $f_x(x,y,t)$ be continuous on $\R^3$, $s \in \R$, and let $\phi: [s-r, s] \to \R$ be continuous history function. Then there exists $\sigma > s$ and a unique solution of the initial-value problem ([-@eq-1d-dde])-([-@eq-1d-dde-initial-cond]) on $[s-r, \sigma]$
:::

:::{.proof}
Follows directly from calculations above.
:::

A careful reader might notice that @thm-1d-single-delay-method-of-steps guarantees only a local solution, but we might want a solution for any $t \geq s$. We will use the notation $[s-r, \sigma\rco$ to denote either the open interval $[s-r, \sigma)$ or the closed on $[s-r, \sigma]$.

From the uniqueness follows that for two solutions $x(t)$ on $[s-r, \sigma\rco$ and $\hat x(t)$ on $[s-r, \rho\rco$, the equality $x(t) = \hat x(t)$ must hold for all $t$, such that both sides of the equality are defined. Moreover, if $[s-r, \sigma\rco \subset [s-r, \rho\rco$, then $\hat x$ is called an *extension* of $x$ and we write $x \subset \hat x$.

One can prove there exists a *unique* maximally defined solution, i.e. one for which there are no extensions, $x : [s-r, \sigma) \to \R$, similarly as for ODEs, for $\sigma \in \R \cup \set{\infty}$. Such solution is called *non-extendable* and is necessarily defined on a right-open interval $[s-r, \sigma)$. Indeed, if $\sigma < \infty$ and $x$ were a solution on $[s-r, \sigma]$, then, by @thm-1d-single-delay-method-of-steps, it could be extended to a larger interval, which contradicts non-extendability of $x$.

:::{#thm-dde-finite-time-blow-up}
##### Finite-time blow-up

Let $f$ satisfy the conditions of @thm-1d-single-delay-method-of-steps and let $x: [s-r, \sigma) \to \R$, $\sigma \in \R \cup \set{\infty}$, be the non-extendable solution of the initial value problem ([-@eq-1d-dde])-([-@eq-1d-dde-initial-cond]). If $\sigma < \infty$, then
$$
\lim_{t \to \sigma^-} \absval{x(t)} = \infty.
$$
:::

:::{.proof}
See @Smith2010, page 26.
:::

::: {.callout-note #nte-general-dde}
##### On multiple delays and $\vi x \in \R^n$
One can rather easily show that @thm-1d-single-delay-method-of-steps and @thm-dde-finite-time-blow-up extend to the case of $\vi x \in \R^n$ and $\vi f : \R \times \R^n \times \R^n \to \R^n$. Similarly, it can also be generalized to multiple discrete delays $r_1 < \dots < r_m$ where
$$
\vi f = \vi f(t, \vi y(t), \vi y(t - r_1), \dots, \vi y(t - r_m))
$$
with little to no change.
:::

<!-- TODO: Write about method of steps in Julia -->

### Notes on Delay Differential Equations

Consider a DDE of form
$$
\dvi x(t) = \vi f(t, \vi x(t), \vi x(t - \tau_1), \dots, \vi x(t - \tau_m)),
$$ {#eq-dde-general}
with $0 = \tau_0 < \tau_1 < \dots < \tau_m$. For brevity, we will denote $\tau := \tau_m$ and
$$
\vi x_t(s) := \vi x(t+s), \; -\tau \leq s \leq 0,
$$
which, informally, prescribes the value of solution $\vi x$ up to time $t$ delayed by $s$. In other words, we can view the trajectory of our solution as the curve $t \to \vi x_t$ in the state space $\contStateSpace = \contf{[-\tau, 0], \R^n}{}$ with supremum norm
$$
\norm{\vi \phi} = \sup_{-\tau \leq s \leq 0} \norm{\vi \phi(s)},
$$
a Banach space of continuous functions mapping the interval $[-\tau, 0]$ to $\R^n$ with the topology of uniform convergence, see @Hale1993. Thus, we can reformulate ([-@eq-dde-general]) as
$$
\dvi x(t) = \vi F(t, \vi x_t),
$$ {#eq-rfde}
which can generally represent any RFDE, with $\vi F : \timeSet \times \contStateSpace \to \R^n$ defined as
$$
\vi F(t, \vi \phi) = \vi f(t, \vi \phi(-\tau_0), \vi \phi(-\tau_1), \dots, \vi \phi(-\tau_m)).
$$ {#eq-dde-functional}

Most often, we will encounter the following *autonomous* initial-value problem
$$
\begin{aligned}
	\dvi x(t) &= \vi f(\vi x(t), \vi x(t - \tau_1), \dots, \vi x(t - \tau_m)), \\
	\vi x_{\sigma} &= \vi \phi,
\end{aligned}
$$ {#eq-dde-problem-autonomous}
where $\sigma \in \R$ is the initial time and $\vi \phi \in \contStateSpace$ is the state of system at time $\sigma$, i.e. the $\tau$-long history function corresponding to the interval $[\sigma - \tau, \sigma]$. For completeness sake, we also add that general RFDE initial-value problem reads as follows
$$
\begin{aligned}
	\dvi x(t) &= \vi F(t, \vi x_t), \\
	\vi x_{\sigma} &= \vi \phi.
\end{aligned}
$$ {#eq-rfde-problem}

Last, but not least, we shall denote by
$$
\jacobi_{\vi \tau_i} \vi f = \partialOp{\vi \xi_{i}} \vi f(t, \overbrace{\vi x(t - \tau)}^{\vi \xi_0}, \overbrace{\vi x(t - \tau_1)}^{\vi \xi_1}, \dots, \overbrace{\vi x(t - \tau_m)}^{\vi \xi_m})
$$
the Jacobian matrix of $\vi f$ corresponding to the $i$-th delayed state input. In other words, we may write the linearization of ([-@eq-dde-problem-autonomous]) at $s$ as
$$
\dvi x(t) = \sum_{i = 0}^m \jacobi_{\vi \tau_i} \vi f(\vi x(s - \tau_0), \dots, \vi x(s - \tau_m)) \vi x(t - \tau_i).
$$ {#eq-dde-linearization}

#### Backward Extension {#sec-backward-extension-dde}

Let us for a moment return to ([-@eq-rfde-problem]). So far, we have discussed its solution for $t > \sigma$, but, sometimes, we may solve it backwards in time for $t < \sigma$, i.e. perform a *backward extension* of the initial history $\phi$. Importantly, it should evident there is a fundamental asymmetry between the past and the future for DDEs, recall they are only a special case of RFDEs, which is non-existent in the case of ODEs. Indeed, as one can see from, for example, @Chicone2006, there is no fundamental distinction between solving forward or backward in time.

Per @Smith2010 and @Hale1993, we say that $\vi x : [\sigma - \tau - \alpha, \sigma] \to \R^n$ for $\alpha > 0$ is a (backward) solution of the initial-value problem given by ([-@eq-rfde-problem]) if $\vi x$ is continuous, $\vi x$ satisfies the initial condition and
$$
\dvi x(t) = \vi F(t, \vi x_t), \; t \in [\sigma - \alpha, \sigma].
$$
Because $\vi x_{\sigma} = \vi \phi$ must hold as well, due to the initial condition, we get that $\vi x(t) = \vi \phi(t - \sigma)$ must be continuously differentiable for $t \in [\sigma - \alpha, \sigma]$. As $\vi \phi \in \contStateSpace$, we can equivalently say that $\vi \phi$ must be continuously differentiable on $[-\minOf{\alpha, \tau}, 0]$. This significantly constraints the space of all admissible elements in $\contStateSpace$. Furthermore, because the equality must hold at $t = \sigma$, we get
$$
\dvi \phi(0) = \dvi x(\sigma) = \vi f(\sigma, \vi x_{\sigma}) = \vi f(\sigma, \vi \phi),
$$
where $\dvi \phi(0)$ denotes the left-hand derivative at $0$. In total, the history function $\vi \phi$ must thus belong to a very limited submanifold $\obj M$ of $\contStateSpace$ where
$$
\obj M = \set{\vi \psi \in \contStateSpace \cap \contf{[-\minOf{\tau, \alpha}, 0], \R^n}{1} : \dvi \psi(0) = \vi f(\sigma, \vi \psi)}.
$$

It is reasonable to assume that for a typical history function $\vi \phi \in \contStateSpace$, these conditions will not be met and therefore backward extension will not be possible. However, if $\vi x: [\sigma - \tau, \sigma + A)$, $A > 0$ is a (forward) solution of ([-@eq-rfde-problem]) and if $\sigma_1 \in (\sigma, \sigma + A)$, then the initial-value problem corresponding to this intermezzo initial condition $(\sigma_1, \vi \psi)$, where $\vi \psi = \vi x_{\sigma_1}$, has a backward solution $\vi x$. It can be shown that many (but not all) solutions of autonomous systems ([-@eq-dde-problem-autonomous]) extend to all $t \in \R$ -- steady-state and periodic solutions can be given as examples.

#### Stability

We have already discussed stability for dynamical systems in @def-invariant-set-stable. Stability for DDEs can be defined analogously, which will be summarized in the following definition (per @Smith2010 and @Hale1993).

:::{#def-dde-stability}
##### Stability in Delay Differential Equations

Consider ([-@eq-rfde-problem]) and suppose that $\vi F(t, \vi 0) = \vi 0$, $t \in \R$, is satisfied so that $\vi x(t) = \vi 0$ is a solution. The solution $\vi x = \vi 0$ is called

- *stable* if for any $\sigma \in \R$ and $\ve > 0$, there exists $\delta = \delta(\sigma, \ve) > 0$ such that $\vi \phi \in \contStateSpace$ and $\norm{\vi \phi} < \delta$ implies that $\norm{\vi x_t(\sigma, \vi \phi)} < \ve$, $t \geq \sigma$;^[For clarity, we use $\vi x(t, \sigma, \vi \phi)$ to denote the solution of ([-@eq-rfde-problem]).]
- *asymptotically stable* if it is stable and if there exists $b(\sigma) > 0$ such that whenever $\vi \phi \in \contStateSpace$ and $\norm{\vi \phi} < b(\sigma)$, then $\vi x(t, \sigma, \vi \phi) \to 0$ as $t \to \infty$;
- *unstable* if it is not stable.
:::

For a non-zero solution $\vi y(t)$ of ([-@eq-rfde-problem]) defined on $t \in \R$, its stability properties mimic those of the zero solution of
$$
\dvi z(t) = \vi f(t, \vi z_t + \vi y_t) - \vi f(t, \vi y_t).
$$ {#eq-non-zero-stability}
Indeed, let $\vi \xi(t)$ be another solution of ([-@eq-rfde-problem]) and put $\vi z(t) = \vi \xi(t) - \vi y(t)$. Then $\vi z_t = \vi \xi_t - \vi y_t$ and thus (by using ([-@eq-rfde-problem]) on $\vi \xi_t$ or $\vi y_t$)
$$
\dvi \xi(t) - \dvi y(t) = \vi f(t, \vi \xi_t - \vi y_t + \vi y_t)  - \vi f (t, \vi y_t),
$$
which yields that $\vi z$ is a solution to ([-@eq-non-zero-stability]), which was to be shown.

### Time Monoids and Dynamical Systems {#sec-time-monoid-dynamical-system}

So far, we have only seen examples of dynamical systems, which have inherently been symmetric in time. In other words, these systems did not distinguish between the future and the past. As an example, consider the following ODE and its induced dynamical system,
$$
\dvi x(t) = \vi f(t, \vi x), \; \vi x(s) = \vi x_0.
$$
When integrating such ODE, it is only our choice to integrate forward in time, but backward integration is just a possible. This corresponds to our requirement on the time structure $\timeSet$ -- we required that $\timeSet$ is a subgroup of $(\R, +)$, thus for every $t \in \timeSet$, there must exists a corresponding inverse $-t \in \timeSet$.

On the other hand, as we have seen in @sec-backward-extension-dde, delay differential equations are not always solvable backward in time. As such, we can relax our requirement of the time structure, this time constraining it to be a monoid over the reals equipped with addition.

:::{#def-semidynamical-system}
##### Semidynamical system

A *semidynamical system* is a triple $\set{\timeSet, \contStateSpace, \evolOp^t}$^[For semidynamical systems, we will use $\contStateSpace$ instead of $\stateSpace$ to emphasize that $x \in \contStateSpace$ will typically be some history function.], where $\timeSet \subseteq \R$ (*time*) endowed with addition $+$ is a monoid, $\contStateSpace$ is a metric space (called a *state space*), and $\set{\evolOp^t}_{t \in \timeSet}$ is a family of evolution operators parametrized by $t \in \timeSet$, such that $\evolOp^t : \contStateSpace \to \contStateSpace$ maps a certain point $x^0 \in \contStateSpace$ to some other state $x_t = \evolOp^t x^0 \in \contStateSpace$.
:::

The difference between @def-dynamical-system and @def-semidynamical-system is, indeed, very subtle. In the case of a dynamical systems, as $\timeSet$ is a subgroup then for any $t \in \timeSet$ and its corresponding $\evolOp^t$, there exists $-t \in \timeSet$ and $\evolOp^{-t}$, giving us (assuming @def-autonomous-dynamical-system) $\evolOp^{-t} \circ \evolOp^t = \evolOp^0 = \id$.  On the other hand, for a semidynamical system with a monoid $\timeSet$, we do not have a guaranteed existence of $-t \in \timeSet$.

Most often, we see semidynamical systems arise by the choice of time set $\timeSet = \N_0$ or $\timeSet = \R^+$. Nevertheless, for DDEs the exact time set, allowed by the equation, of the induced semidynamical system is often more nuanced. Moreover, autonomous deterministic semidynamical system is defined analogously to @def-deterministic-dynamical-system and @def-autonomous-dynamical-system. Basic concepts defined for dynamical systems, see @sec-basic-concepts, mostly extend to semidynamical systems (for example, we only consider $\omega$-limit point and set).

:::{#lem-autonomous-semidynamical-system}
Deterministic semidynamical system $\set{\timeSet, \contStateSpace, \evolOp^t}$ is autonomous if and only if whenever $\vi x(t)$ is a solution defined on subset $I \subseteq \timeSet$ and $\tau \in \timeSet$, then $\vi x(t + \tau)$ is a solution on $I - \tau$, where $I - \tau = \set{t - \tau \divider t \in I}$.
:::

:::{.proof}
Can be adapted from @Smith2010, page 63.
:::

Analogously to the dynamical system case, the family of evolution operators $\set{\evolOp^t}_{t \in \timeSet}$ of an autonomous semidynamical system is called a *semiflow*.

#### Semidynamical Systems Induced by Delay Differential Equations

For compatibility with the source material, see @Smith2010, we will now consider an initial-value problem ([-@eq-rfde-problem]) for a RFDE, though for our purposes a DDE would suffice,
$$
\dvi x(t) = \vi F(t, \vi x_t), \; \vi x_\sigma = \vi \phi,
$$
where $\sigma \in \R$, $\vi F$ is continuous, and $\vi \phi \in \contStateSpace$. Let us assume there is a unique solution $\vi x(t, \sigma, \vi \phi)$ of ([-@eq-rfde-problem]) defined for $t \geq \sigma$. While this condition is hard to ensure in practice, it is nonetheless necessary for the definition of the induced semidynamical system. Denote by $\vi x_t(\sigma, \vi \phi) \in \contStateSpace$ the state of the system at time $t$, defined, as usual, by
$$
\vi x_t(\sigma, \vi \phi)(\tht) = \vi x(t + \theta, \sigma, \vi \phi), \; -\tau \leq \tht \leq 0.
$$

:::{#prp-dde-induced-semidynamical-system}
Operator $\evolOp^t = \vi x_t(\sigma, \cdot)$ defines a semidynamical system on $\contStateSpace$ with $\timeSet = [\sigma - \tau, \infty)$.
:::

:::{.proof}
See @Smith2010, page 65.
:::

## Numerical Methods

As the title may suggest, computational aspects of ODEs, DDEs and other mathematical problems will play a key role in this thesis. For completeness' sake, a brief overview of numerical integrators (also called *solvers*) for ODEs, DDEs and stochastic differential equations (SDEs) will be given, as well as basic methods of one-dimensional optimization. Lastly, Newton-Raphson method will be discussed.
### Numerical Integration

#### Solving ODEs

Solving, or numerically integrating, ordinary differential equations can be seen as a backbone of this thesis. As such, we will introduce two approaches, Euler method and Runge-Kutta method. This subsection is mostly based on @Butcher2016, @Hairer2008 and @Trefethen1994.

Consider an ODE of form
$$
\dvi x(t) = \vi F(t, \vi x(t)), \; \vi x(t_0) = \vi x_0,
$$ {#eq-ode-problem-to-solve}
which we want to integrate on the interval $[t_0, t_N]$. By Taylor expansion at $t_0$, we can write
$$
\vi x(t) = \vi x(t_0) + \dvi x(t_0) (t - t_0) + O(t^2).
$$ {#eq-euler-method-taylor}
Setting $t = t_0 + \diff t_1$ yields 
$$
\vi x(t_0 + \diff t_1) \approx \vi x(t_0) + \dvi x(t_0) (t_0 + \diff t_1  - t_0) = \vi x(t_0) + \vi F(t_0, \vi x(t_0)) \diff t_1.
$$

Repeating this process on the entire interval $[t_0, t_N]$ gives us @alg-euler-method. Let us note that often an equidistant stepsize is used, e.g. $\diff t_k = \diff t = \frac {t_N - t_0} N$.

::: {.callout-important appearance="simple" #alg-euler-method}
##### Euler method (explicit)
**Input:** ODE in form ([-@eq-ode-problem-to-solve]), stepsizes $\seqnc{\diff t}{i}{1}{N}$ \
**Output:** Sequence of $N$ points $\seqnc{\vi x}{i}{1}{N}$ \
**begin** \
&emsp;&emsp;**for** $i = 1$ **to** $N$ **do** \
&emsp;&emsp;&emsp;&emsp;$\vi x_i \letDef \vi x_{i - 1} + \vi F(t_{i - 1}, \vi x_{i - 1}) \diff t_i$ \
&emsp;&emsp;**end** \
**end**
:::

Whilst the Euler method is simple, it does have its drawbacks mainly in the accuracy. For quantification of this issue, we shall define several notions of errors of a numerical integration method.

:::{#def-ode-solver-error}
Let us consider an ODE problem of form ([-@eq-ode-problem-to-solve]) and the solution $\vi x(t)$ to the initial-value problem. Assume $\vi x_k$ is an approximate solution at time $t_k$ and that $\vi x_{k+1}$ is given by $\vi x_{k+1} = \vi x_k + \diff t_{k+1} \cdot \incrementF{t_k, \vi x (t_k), \diff t_{k+1}, \vi F}$, where $\incrementFunc$ is the *increment* function^[In general, the increment function $\incrementFunc$ at the $k+1$-st step may depend on $t_0, \dots, t_k$, $\vi x_0, \dots, \vi x_k$, and $\diff t_1, \dots, \diff t_{k + 1}$.], which propagates the solution from $t_k$ to $t_{k+1}$ and represents the chosen method. 

*Local truncation error* (LTE) is defined as
$$
\locTrErr_{k} = \vi x(t_{k+1}) - \vi x(t_k) - \diff t_{k+1} \cdot \incrementF{t_k, \vi x (t_k), \diff t_{k+1}, \vi F}
$$ {#eq-loc-tr-err}
and represents the error the increment function causes when performing a single step. Similarly, *global error* is defined as the cummulative LTE from the initial condition,
$$
\globErr_k = \vi x(t_k) - \vi x_k.	
$$ {#eq-glob-err}
:::

> In general, we seek such method, which fulfilles given accuracy while using as few evaluations of $\vi F$ as possible -- the assumption is that the evaluation of $\vi F$ is so costly the remaining operations are almost negligible.

By using ([-@eq-loc-tr-err]) on @alg-euler-method to obtain LTE for Euler method, we get (recalling the Taylor expansion ([-@eq-euler-method-taylor]))
$$
\locTrErr_k = \vi x(t_{k+1}) - \vi x(t_k) - \diff t_{k+1} \cdot \vi F(t_k, \vi x(t_k)) \implies \locTrErr_k = O(\diff t_{k+1}^2).
$$
We say that a method is of order $p$ if $\locTrErr_k = O(\diff t^{p+1})$, i.e. Euler method is first order.

This can surely be improved by using more terms from the Taylor series expansions, see ([-@eq-euler-method-taylor]), but the necessity of computing the derivatives of $\vi F$ is a major drawback, often outweighing any potential benefits. Another possibility would be to allow to computation of next value to be dependent on multiple previous values, leading to multistep methods. Last option --- and the one we shall use --- is to modify a one-step method to produce a intermediate estimates of the solution at multiple different points (called *stages*), which are then combined into the final approximation of the solution at the next step -- this way, we obtain so called **Runge-Kutta** methods.

::: {.callout-important appearance="simple" #alg-runge-kutta-explicit}
##### $s$-stage explicit Runge-Kutta method
**Input:** ODE in form ([-@eq-ode-problem-to-solve]), stepsizes $\seqnc{\diff t}{i}{1}{N}$, $s \in \N$ (the "number of stages"), real coefficients $a_{21}, a_{31}, a_{32},\dots, a_{s1}, a_{s2}, \dots, a_{s,s-1}, b_1, \dots, b_s, c_2, \dots, c_s$\
**Output:** Sequence of $N$ points $\seqnc{\vi x}{i}{1}{N}$ \
**begin** \
&emsp;&emsp;**for** $i = 1$ **to** $N$ **do** \
&emsp;&emsp;&emsp;&emsp;calculate
\begin{align*}
	\vi k_1 &= \vi F (t_0, \vi x_{i-1}), \\
	\vi k_2 &= \vi F (t_0 + c_2 \diff t_i, \vi x_{i-1} + \diff t_i a_{21} \vi k_1), \\
	\vi k_3 &= \vi F (t_0 + c_3 \diff t_i, \vi x_{i-1} + \diff t_i (a_{31} \vi k_1 + a_{32} \vi k_2)), \\
	&\;\vdots \\
	\vi k_s &= \vi F (t_0 + c_s \diff t_i, \vi x_{i-1} + \diff t_i (a_{s1} \vi k_1 + \dots + a_{s, s-1} \vi k_{s-1}))
\end{align*}
&emsp;&emsp;&emsp;&emsp;set $\vi x_i \letDef \vi x_{i-1} + \diff t_i (b_1 \vi k_1 + \dots + b_s \vi k_s)$ \
&emsp;&emsp;**end** \
**end**
:::

<!-- Newton method with \diff x, see: https://www.iue.tuwien.ac.at/phd/orio/node55.html -->